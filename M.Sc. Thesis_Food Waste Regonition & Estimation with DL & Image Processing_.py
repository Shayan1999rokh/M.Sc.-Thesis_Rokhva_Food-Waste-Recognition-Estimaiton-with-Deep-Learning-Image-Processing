# -*- coding: utf-8 -*-
"""Rokhva_Thesis_Food_Waste_Estimation_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pck1WtVonu-50M23zGDbHJLHE7UmNxSF

# **Shayan Rokhva_M.Sc Thesis_Food_Waste Recognition & Estimation_**
#### Contact: Shayanrokhva1999@gmail.com  & Shayan1999rokh@yahoo.com
#### Attention: During the prepreation of this code, AI has been used responsibly to generatee, dubug and documentation. We have reviwed the generated/altered code by AI and we take responsibility.

## ðŸ“¦ Imports and Inits
"""

from google.colab import drive
drive.mount('/content/gdrive')

!nvidia-smi

# Some of them may not be required
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

import os
import glob
import zipfile

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.backends.cudnn as cudnn
from   torch.utils.data import Dataset, DataLoader

import torchvision
import torchvision.models as models
import torchvision.transforms as T
import torchvision.datasets as datasets

!pip install tqdm
from tqdm import tqdm

from PIL  import Image

!pip install torchmetrics
import torchmetrics
from torchmetrics import Accuracy, F1Score, Precision, Recall

from sklearn.metrics import confusion_matrix, accuracy_score
from tabulate import tabulate

import torch
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm import tqdm
import torch.nn as nn
import torch.optim as optim

# import tensorflow as tf
# from tensorflow.keras import layers, models

import os
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
!pip install albumentations

!pip install torchmetrics

!pip install -U torchmetrics

from google.colab import drive
drive.mount('/content/drive')

"""## ðŸŒ± Seeds for Reproducability (commented)"""

# import random
# import numpy as np
# import torch

# # Set a fixed seed for reproducibility
# seed = 42

# # Set Python's built-in random seed
# random.seed(seed)

# # Set NumPy random seed
# np.random.seed(seed)

# # Set PyTorch random seed for CPU
# torch.manual_seed(seed)

# # Set PyTorch random seed for all GPUs
# torch.cuda.manual_seed(seed)
# torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU

# # Ensures that CUDA algorithms are deterministic (potentially at the cost of speed)
# torch.backends.cudnn.deterministic = True
# torch.backends.cudnn.benchmark = False

# # DataLoader settings (in your DataLoader initialization)
# # DataLoader(..., shuffle=False, num_workers=0) # Replace with your actual DataLoader settings

"""## ðŸ“¥ Data Loading

You will say what food type would you like to be loaded and then, we will load that from Google Drive.
"""

"""
Cell 1: Ask user which food category they want to analyze (NO numbering, exact name input only)

This block asks the user to type one of the predefined food category names exactly.
If the input is valid, it confirms the selected category.
If invalid, it asks the user to rerun the cell.
"""

# List of valid food category folder names
valid_categories = [
    "AdasPolo",
    "CheloGoosht",
    "Fesenjan",
    "GheymeBademjan",
    "Protein_and_Fries"
]

# Ask for user input (case-sensitive, no numbering)
selected_category = input(
    "Which food category would you like to analyze?\n"
    "Type exactly one of the following:\n"
    "AdasPolo, CheloGoosht, Fesenjan, GheymeBademjan, Protein_and_Fries\n\n"
    "Your choice: "
)

# Validate input
if selected_category in valid_categories:
    print(f"âœ… You have selected: {selected_category}. You can continue")
else:
    print("âŒ Invalid input. Please re-run this cell and type the name exactly as shown.")
    selected_category = None  # Mark as invalid for next cells

"""
Cell 2: Copy only the selected food category folders from Google Drive to local Colab path

This cell assumes the user has already selected a valid category in Cell 1.
If `selected_category` is valid, only the corresponding train/test folders will be copied to `/content/`.
"""

import os

if selected_category:
    # Build source paths based on user selection
    train_src = f"/content/drive/MyDrive/Foods_Train&Test_256/Train_{selected_category}_256"
    test_src = f"/content/drive/MyDrive/Foods_Train&Test_256/Test_{selected_category}_256"

    # Build destination path
    dst_base = "/content/"

    # Copy using shell commands
    os.system(f'cp -r "{train_src}" "{dst_base}"')
    os.system(f'cp -r "{test_src}" "{dst_base}"')

    print(f"âœ… Copied folders for {selected_category} to /content/")
else:
    print("âš ï¸ Skipping copy. No valid category selected in Cell 1. Please rerun Cell 1.")

"""## ðŸ§¹Data Prepration

### Checking for problemtic masks (None detected. Yet, still beneficial)
"""

"""
Cell 3: Check class consistency in mask files based on the selected food category

- Automatically adjusts expected number of classes based on the selected category
- Reports how many masks exceed the expected number of unique class labels
"""

import os
import numpy as np
from PIL import Image

# === Make sure a valid selection was made ===
if selected_category is None:
    print("âš ï¸ No valid food category selected. Please rerun Cell 1.")
else:
    # === Paths ===
    train_mask_dir = f"/content/Train_{selected_category}_256"
    test_mask_dir = f"/content/Test_{selected_category}_256"

    # === Expected number of classes (including background) ===
    expected_classes = 2 if selected_category == "AdasPolo" else 3

    # === Counters ===
    train_total = 0
    train_problematic = 0
    test_total = 0
    test_problematic = 0

    # === Check train masks ===
    for fname in os.listdir(train_mask_dir):
        if fname.endswith(".png") and "mask" in fname.lower():
            mask_path = os.path.join(train_mask_dir, fname)
            mask = np.array(Image.open(mask_path).convert("L"))
            unique_classes = np.unique(mask)

            train_total += 1
            if len(unique_classes) > expected_classes:
                train_problematic += 1

    # === Check test masks ===
    for fname in os.listdir(test_mask_dir):
        if fname.endswith(".png") and "mask" in fname.lower():
            mask_path = os.path.join(test_mask_dir, fname)
            mask = np.array(Image.open(mask_path).convert("L"))
            unique_classes = np.unique(mask)

            test_total += 1
            if len(unique_classes) > expected_classes:
                test_problematic += 1

    # === Results ===
    print(f"âœ… Selected category: {selected_category}")
    print(f"Expected number of classes (including background): {expected_classes}")
    print("")
    print(f"Train - Total masks checked: {train_total}")
    print(f"Train - Problematic masks (>{expected_classes} classes): {train_problematic}")
    print("")
    print(f"Test  - Total masks checked: {test_total}")
    print(f"Test  - Problematic masks (>{expected_classes} classes): {test_problematic}")

"""### Alignment of images and masks"""

"""
Cell 4: Check alignment between images and masks for the selected food category

- Dynamically uses the selected category
- Compares `.jpg` image filenames with `.png` mask filenames (after removing `_mask`)
- Reports if filenames are mismatched
"""

import os

# === Safety check ===
if selected_category is None:
    print("âš ï¸ No valid food category selected. Please rerun Cell 1.")
else:
    # === Paths ===
    train_dir = f"/content/Train_{selected_category}_256"
    test_dir = f"/content/Test_{selected_category}_256"

    def check_alignment(data_dir, label):
        # Collect files
        image_files = sorted([f for f in os.listdir(data_dir) if f.endswith(".jpg")])
        mask_files = sorted([f for f in os.listdir(data_dir) if f.endswith(".png") and "mask" in f.lower()])

        # Normalize names (remove extensions, and "_mask" from mask filenames)
        image_basenames = set(os.path.splitext(f)[0] for f in image_files)
        mask_basenames = set(os.path.splitext(f.replace("_mask", ""))[0] for f in mask_files)

        # Compare sets
        aligned = image_basenames == mask_basenames

        # Output summary
        print(f"{label} - Total images: {len(image_files)}")
        print(f"{label} - Total masks:  {len(mask_files)}")
        print(f"{label} - Alignment OK: {aligned}")

        # If mismatch, show details
        if not aligned:
            missing_in_masks = image_basenames - mask_basenames
            missing_in_images = mask_basenames - image_basenames
            if missing_in_masks:
                print(f"{label} - Images without masks: {sorted(missing_in_masks)}")
            if missing_in_images:
                print(f"{label} - Masks without images: {sorted(missing_in_images)}")

    # === Run checks ===
    check_alignment(train_dir, "Train")
    print("-" * 50)
    check_alignment(test_dir, "Test")

"""
Cell 5: Summarize file counts for selected food category's train/test folders

- Counts total files, image files (.jpg), and mask files (.png)
- Fully dynamic using `selected_category`
"""

import os

# === Safety check ===
if selected_category is None:
    print("âš ï¸ No valid food category selected. Please rerun Cell 1.")
else:
    # === Define dynamic folder paths based on selected category ===
    folders = {
        "Train Images": f"/content/Train_{selected_category}_256",
        "Test Images": f"/content/Test_{selected_category}_256"
    }

    # === Loop through each folder and summarize contents ===
    for label, path in folders.items():
        all_files = os.listdir(path)
        jpg_files = [f for f in all_files if f.lower().endswith(".jpg")]
        png_files = [f for f in all_files if f.lower().endswith(".png")]

        print(f"\nðŸ“ {label} ({path})")
        print(f"ðŸ“¦ Total files: {len(all_files)} â€” images + masks")
        print(f"ðŸ–¼ï¸  JPG images: {len(jpg_files)}")
        print(f"ðŸŽ­ PNG masks:  {len(png_files)}")
        # Optional: Show sample filenames
        # print(f"ðŸ“ Sample JPGs: {jpg_files[:2]}")
        # print(f"ðŸ“ Sample PNGs: {png_files[:2]}")

"""## ðŸ—ºï¸ Custom Class for Semantic Segmentation"""

"""
Custom Dataset Class for Semantic Segmentation of Food Categories

- Dynamically supports any selected food category (paths passed manually)
- Ensures sorted loading of `.jpg` images and corresponding `_mask.png` masks
- Handles optional Albumentations transforms
- Ensures grayscale masks and adds channel dimension
- Converts to PyTorch tensors if no transform is provided
"""

import os
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
import torch

class SegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir, transform=None):
        """
        Args:
            image_dir (str): Path to folder containing `.jpg` images
            mask_dir (str): Path to folder containing `_mask.png` masks
            transform (callable, optional): Albumentations-style transform
        """
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform

        # Sorted list of image filenames
        self.images = sorted([
            f for f in os.listdir(self.image_dir)
            if f.lower().endswith(".jpg")
        ])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # --- File paths ---
        img_filename = self.images[idx]
        img_path = os.path.join(self.image_dir, img_filename)

        base_name = os.path.splitext(img_filename)[0]
        mask_filename = base_name + "_mask.png"
        mask_path = os.path.join(self.mask_dir, mask_filename)

        # --- File existence check ---
        assert os.path.exists(mask_path), f"âŒ Mask not found for image: {img_filename}"

        # --- Load image and mask ---
        image = Image.open(img_path).convert("RGB")
        mask = Image.open(mask_path).convert("L")  # Grayscale

        # Convert to NumPy arrays
        image = np.array(image)
        mask = np.array(mask)

        # Safety: reduce to single channel if needed
        if mask.ndim == 3:
            mask = mask[..., 0]

        # --- Apply transforms if defined ---
        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented["image"]
            mask = augmented["mask"]
        else:
            # Convert to PyTorch tensors manually
            image = torch.tensor(image).permute(2, 0, 1).float() / 255.0  # [C, H, W]
            mask = torch.tensor(mask).long()  # [H, W]

        # Add channel dimension to mask: [1, H, W]
        if mask.ndim == 2:
            mask = mask.unsqueeze(0)

        return image, mask

"""## ðŸ§ªTransformation & Augmentation"""

import albumentations as A
import numpy as np
import random
import torch

# Parameters
image_size = 256                            # Resize image size
imagenet_mean = [0.485, 0.456, 0.406]       # ImageNet means for normalization
imagenet_std  = [0.229, 0.224, 0.225]       # ImageNet standard deviations for normalization

# Main transformation without augmentations (fixed transformation)
main_transf = A.Compose([
                          A.Resize(height=image_size, width=image_size),
                          A.Normalize(mean=imagenet_mean, std=imagenet_std),
                          A.pytorch.ToTensorV2(),                 # For PyTorch compatibility
                         ])

# Augmentation transformation with random transformations (seeded for reproducibility)
augm_transf = A.Compose([
                          A.Resize(height=image_size, width=image_size),
                          A.Rotate(limit=60, p=0.5),
                          A.HorizontalFlip(p=0.5),
                          A.VerticalFlip(p=0.25),
                          A.Normalize(mean=imagenet_mean, std=imagenet_std),
                          A.pytorch.ToTensorV2(),                 # For PyTorch compatibility
                        ])


# For this study
# ===============================================================================================================================================================
# Data Augmentation is inherently aavailable in the training set using Roborflow. So, this data variation just makes the data more varied and does not increase data quantity.
# Furthermore, this is a very very light data augmentation. Just includes some rotations and flipping.
# While we do not apply that on valid/test, the alterations are so light that will make no signficant change. Slight alterations in the traning set may still be beneficial.
# As mentioned, the real augementation is inherent in the dataset (training set)

"""## ðŸ‹ï¸â€â™‚ï¸ðŸ“ŠTrain/Valid/Test"""

"""
Cell 1: Safely splits the selected food category's training data into 85% train + 15% validation.
- Dynamically uses selected_category
- Ensures mask-image alignment
"""

import os
import shutil
import random
from PIL import Image

# Configuration
seed = 42
random.seed(seed)

if selected_category is None:
    print("âš ï¸ No valid category selected. Rerun Cell 1.")
else:
    src_dir = f"/content/Train_{selected_category}_256"
    base_path = "/content"

    # Output folders
    split_folders = {
        "only_train_images": os.path.join(base_path, "only_train_images"),
        "only_train_masks": os.path.join(base_path, "only_train_masks"),
        "valid_images": os.path.join(base_path, "valid_images"),
        "valid_masks": os.path.join(base_path, "valid_masks"),
    }

    # Clean/create folders
    for path in split_folders.values():
        if os.path.exists(path):
            shutil.rmtree(path)
        os.makedirs(path)

    # Match image-mask pairs
    all_files = os.listdir(src_dir)
    img_files = sorted([f for f in all_files if f.lower().endswith(".jpg")])
    mask_files = set([f for f in all_files if f.lower().endswith(".png") and "_mask" in f])

    valid_pairs = []
    for img_file in img_files:
        base = os.path.splitext(img_file)[0]
        mask_name = base + "_mask.png"
        if mask_name in mask_files:
            valid_pairs.append((img_file, mask_name))

    # Shuffle and split
    random.shuffle(valid_pairs)
    split_idx = int(0.85 * len(valid_pairs))
    train_pairs = valid_pairs[:split_idx]
    val_pairs = valid_pairs[split_idx:]

    # Copying
    def copy_pairs(pairs, src, dst_img, dst_mask):
        for img_file, mask_file in pairs:
            shutil.copy(os.path.join(src, img_file), os.path.join(dst_img, img_file))
            shutil.copy(os.path.join(src, mask_file), os.path.join(dst_mask, mask_file))

    copy_pairs(train_pairs, src_dir, split_folders["only_train_images"], split_folders["only_train_masks"])
    copy_pairs(val_pairs, src_dir, split_folders["valid_images"], split_folders["valid_masks"])

    print(f"âœ… 85/15 split completed for {selected_category}. Total pairs: {len(valid_pairs)}")

    # Verify alignment
    def verify_alignment(img_folder, mask_folder):
        imgs = sorted([f for f in os.listdir(img_folder) if f.lower().endswith(".jpg")])
        masks = sorted([f for f in os.listdir(mask_folder) if f.lower().endswith(".png") and "_mask" in f])
        img_base = [os.path.splitext(f)[0] for f in imgs]
        mask_base = [os.path.splitext(f.replace("_mask", ""))[0] for f in masks]
        matched = img_base == mask_base
        return matched, len(imgs)

    for ikey, mkey in [("only_train_images", "only_train_masks"), ("valid_images", "valid_masks")]:
        matched, count = verify_alignment(split_folders[ikey], split_folders[mkey])
        print(f"ðŸ” Alignment {ikey} â†” {mkey}: Matched = {matched}, Pairs = {count}")

"""
Cell 2: Define key dataset paths for training, validation, and testing

- Automatically adapts to selected_category
"""

import os

base_path = "/content"
only_train_images = os.path.join(base_path, "only_train_images")
only_train_masks  = os.path.join(base_path, "only_train_masks")
valid_images      = os.path.join(base_path, "valid_images")
valid_masks       = os.path.join(base_path, "valid_masks")
test_images       = os.path.join(base_path, f"Test_{selected_category}_256")
test_masks        = os.path.join(base_path, f"Test_{selected_category}_256")

"""
Cell 3: Analyze file counts and verify image-mask pair alignment

- Checks total files, relevant file types, and name consistency
"""

# Folders to analyze
folders = {
    "only_train_images": only_train_images,
    "only_train_masks": only_train_masks,
    "valid_images": valid_images,
    "valid_masks": valid_masks,
    "test_images": test_images,
    "test_masks": test_masks,
}

# Folder analysis
def analyze_folder(name, path):
    all_files = sorted(os.listdir(path))
    if "mask" in name:
        relevant_files = [f for f in all_files if f.lower().endswith(".png")]
    else:
        relevant_files = [f for f in all_files if f.lower().endswith(".jpg")]
    print(f"\nðŸ“ {name} ({path})")
    print(f"ðŸ“¦ Total files: {len(all_files)}")
    print(f"ðŸ§© Relevant files: {len(relevant_files)}")

for name, path in folders.items():
    analyze_folder(name, path)

# Pair verification
def verify_pairs(img_folder, mask_folder):
    img_files = sorted([f for f in os.listdir(img_folder) if f.endswith(".jpg")])
    mask_files = sorted([f for f in os.listdir(mask_folder) if f.endswith(".png") and "_mask" in f])
    img_bases = [os.path.splitext(f)[0] for f in img_files]
    mask_bases = [os.path.splitext(f.replace("_mask", ""))[0] for f in mask_files]
    matched = img_bases == mask_bases
    print(f"\nðŸ”— {os.path.basename(img_folder)} â†” {os.path.basename(mask_folder)}")
    print(f"âœ… Matched base names: {matched}")
    print(f"ðŸ”¢ Total pairs: {len(img_bases)}")
    if not matched:
        diffs = set(img_bases).symmetric_difference(set(mask_bases))
        print(f"âš ï¸  Unmatched items (sample): {list(diffs)[:5]}")

verify_pairs(only_train_images, only_train_masks)
verify_pairs(valid_images, valid_masks)

"""
Cell 4: Define test set paths based on selected category (used if needed later)
"""

test_images = f"/content/Test_{selected_category}_256"
test_masks  = f"/content/Test_{selected_category}_256"

"""## ðŸ½ï¸Segmentation Dataset"""

"""
Cell 6: Create SegmentationDataset instances for train, valid, and test sets
- Uses category-aware paths
- Applies appropriate transforms:
    - train: uses augmentation
    - valid/test: uses base transformation
"""

# Must ensure selected_category and paths are set from earlier cells

# Create datasets
train_dataset = SegmentationDataset(
    image_dir=only_train_images,
    mask_dir=only_train_masks,
    transform=augm_transf  # With augmentation
)

valid_dataset = SegmentationDataset(
    image_dir=valid_images,
    mask_dir=valid_masks,
    transform=main_transf  # No augmentation
)

test_dataset = SegmentationDataset(
    image_dir=test_images,
    mask_dir=test_masks,
    transform=main_transf  # No augmentation
)

"""
Cell 7: Print the number of samples in each dataset (train, valid, test)
- Helps confirm splits and verify dataset integrity
"""

print(f"ðŸ“¦ Train Dataset size: {len(train_dataset)} samples")
print(f"ðŸ“¦ Valid Dataset size: {len(valid_dataset)} samples")
print(f"ðŸ“¦ Test  Dataset size: {len(test_dataset)} samples")

"""## ðŸŒ€Data Loader + Check"""

"""
Cell 8: Define PyTorch DataLoaders for training, validation, and testing
- Uses batch size = 4, no shuffling for reproducibility
- num_workers = 0 to avoid multiprocessing issues in Colab
"""

from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False, num_workers=0)
valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0)
test_loader  = DataLoader(test_dataset , batch_size=4, shuffle=False, num_workers=0)

print(f"ðŸ§ª Len train_loader: {len(train_loader)}")
print(f"ðŸ§ª Len valid_loader: {len(valid_loader)}")
print(f"ðŸ§ª Len test_loader:  {len(test_loader)}")

"""
Cell 9: Visual inspection of batch shapes for train, valid, and test loaders
- Prints (BatchSize, Channels, H, W) for both images and masks
"""

# Train loader
for images, masks in train_loader:
    print(f"[Train] Image shape: {images.shape}, Mask shape: {masks.shape}")
    break

# Valid loader
for images, masks in valid_loader:
    print(f"[Valid] Image shape: {images.shape}, Mask shape: {masks.shape}")
    break

# Test loader
for images, masks in test_loader:
    print(f"[Test ] Image shape: {images.shape}, Mask shape: {masks.shape}")
    break

"""## ðŸ·ï¸Linking Indices"""

"""
Cell 10: Define class label mappings dynamically based on selected food category
- Returns a dictionary mapping pixel values to semantic class names
- Used for visualization, reporting, or debugging
"""

# Define class mappings for each category (pixel_value: label)
category_class_mappings = {
    "AdasPolo": {
        0: "Background",
        1: "AdasPolo"
    },
    "CheloGoosht": {
        0: "Background",
        1: "Meat_CheloGoosht",
        2: "Rice_CheloGoosht"
    },
    "Fesenjan": {
        0: "Background",
        1: "Fesenjan_Stew",
        2: "Rice_Fesenjan"
    },
    "GheymeBademjan": {
        0: "Background",
        1: "GheymeBademjan_Stew",
        2: "Rice_GheymeBademjan"
    },
    "Protein_and_Fries": {
        0: "Background",
        1: "Fries",
        2: "Protein"
    }
}

# Safety check and dynamic assignment
if selected_category in category_class_mappings:
    class_mapping = category_class_mappings[selected_category]
    print(f"âœ… Class mapping loaded for {selected_category}:")
    for k, v in class_mapping.items():
        print(f"  {k}: {v}")
else:
    class_mapping = None
    print("âŒ Unknown food category. Please check your selection in Cell 1.")

"""## ðŸ–¼ï¸Visualizing a batch

I have made these parts comment for rapid processing. Uncomment them if you like. The codes are accurate

### Train batch
"""

"""
Cell 11: Visualize a batch from the training set
- Uses unnormalize function and food-specific colormap
- Deterministic batch selection using `next(iter(...))`
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
from matplotlib.colors import ListedColormap

# === Colormap Setup ===
color_palettes = {
    "AdasPolo":         ["#000000", "#E69F00"],
    "CheloGoosht":      ["#000000", "#D55E00", "#F0E442"],
    "Fesenjan":         ["#000000", "#009E73", "#56B4E9"],
    "GheymeBademjan":   ["#000000", "#CC79A7", "#0072B2"],
    "Protein_and_Fries":["#000000", "#E69F00", "#56B4E9"],
}

if selected_category in color_palettes:
    custom_colors = color_palettes[selected_category]
    fixed_cmap = ListedColormap(custom_colors)
else:
    raise ValueError("âŒ Unknown category for colormap.")

# === Unnormalize function ===
def unnormalize_image(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):
    tensor = tensor.clone()
    for t, m, s in zip(tensor, mean, std):
        t.mul_(s).add_(m)
    return tensor

# === Visualize a batch from train_loader ===
images, masks = next(iter(train_loader))

num_samples = min(4, images.size(0))
fig, axs = plt.subplots(2, num_samples, figsize=(4 * num_samples, 5))

for i in range(num_samples):
    img = unnormalize_image(images[i])
    img = img.permute(1, 2, 0).cpu().numpy()
    img = np.clip(img * 255, 0, 255).astype(np.uint8)

    mask = masks[i].squeeze().cpu().numpy()

    axs[0, i].imshow(img)
    axs[0, i].set_title(f"Train Image {i+1}")
    axs[0, i].axis("off")

    axs[1, i].imshow(mask, cmap=fixed_cmap, vmin=0, vmax=len(custom_colors)-1)
    axs[1, i].set_title(f"Train Mask {i+1}")
    axs[1, i].axis("off")

plt.tight_layout()
plt.show()

"""### Valid batch"""

"""
Cell 12: Visualize a batch from the validation set
- Uses unnormalize function and food-specific colormap
- Deterministic batch selection using `next(iter(...))`
"""

# === Visualize a batch from valid_loader ===
images, masks = next(iter(valid_loader))

num_samples = min(4, images.size(0))
fig, axs = plt.subplots(2, num_samples, figsize=(4 * num_samples, 5))

for i in range(num_samples):
    img = unnormalize_image(images[i])
    img = img.permute(1, 2, 0).cpu().numpy()
    img = np.clip(img * 255, 0, 255).astype(np.uint8)

    mask = masks[i].squeeze().cpu().numpy()

    axs[0, i].imshow(img)
    axs[0, i].set_title(f"Val Image {i+1}")
    axs[0, i].axis("off")

    axs[1, i].imshow(mask, cmap=fixed_cmap, vmin=0, vmax=len(custom_colors)-1)
    axs[1, i].set_title(f"Val Mask {i+1}")
    axs[1, i].axis("off")

plt.tight_layout()
plt.show()

"""### Test batch"""

"""
Cell 13: Visualize a batch from the test set
- Uses unnormalize function and food-specific colormap
- Deterministic batch selection using `next(iter(...))`
"""

# === Visualize a batch from test_loader ===
images, masks = next(iter(test_loader))

num_samples = min(4, images.size(0))
fig, axs = plt.subplots(2, num_samples, figsize=(4 * num_samples, 5))

for i in range(num_samples):
    img = unnormalize_image(images[i])
    img = img.permute(1, 2, 0).cpu().numpy()
    img = np.clip(img * 255, 0, 255).astype(np.uint8)

    mask = masks[i].squeeze().cpu().numpy()

    axs[0, i].imshow(img)
    axs[0, i].set_title(f"Test Image {i+1}")
    axs[0, i].axis("off")

    axs[1, i].imshow(mask, cmap=fixed_cmap, vmin=0, vmax=len(custom_colors)-1)
    axs[1, i].set_title(f"Test Mask {i+1}")
    axs[1, i].axis("off")

plt.tight_layout()
plt.show()

"""## âœ…Final Checking of batches size"""

"""
Cell 14: Inspect shape and type of one batch from each loader (train, valid, test)
- Verifies tensor structure for model compatibility
- Uses `next(iter(...))` to ensure repeatable results
"""

import torch

# === Train Loader ===
images, masks = next(iter(train_loader))
print("ðŸ“¦ Train Loader:")
print(f"Images shape:     {images.shape} | type: {type(images)}")
print(f"Masks shape:      {masks.shape}  | type: {type(masks)}")

print("")

# === Validation Loader ===
images, masks = next(iter(valid_loader))
print("ðŸ“¦ Validation Loader:")
print(f"Images shape:     {images.shape} | type: {type(images)}")
print(f"Masks shape:      {masks.shape}  | type: {type(masks)}")

print("")

# === Test Loader ===
images, masks = next(iter(test_loader))
print("ðŸ“¦ Test Loader:")
print(f"Images shape:     {images.shape} | type: {type(images)}")
print(f"Masks shape:      {masks.shape}  | type: {type(masks)}")

"""## âš™ï¸**Device**"""

device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""## **ðŸ”¢ In and Out channels**"""

"""
Cell 15: Set in_channels and out_channels based on selected food category
- in_channels is always 3 (RGB)
- out_channels depends on number of mask classes (including background)
"""

# Input always RGB
in_channels = 3

# Output channels based on food category
if selected_category == "AdasPolo":
    out_channels = 2  # Background + AdasPolo
elif selected_category in ["CheloGoosht", "Fesenjan", "GheymeBademjan", "Protein_and_Fries"]:
    out_channels = 3  # Background + 2 foreground classes
else:
    raise ValueError("âŒ Unknown food category for setting out_channels.")

print(f"âœ… Configuration set:")
print(f"in_channels  = {in_channels}")
print(f"out_channels = {out_channels} (based on {selected_category})")

"""
Cell 15: âœ… Set input/output channels for segmentation based on food category

This cell ensures consistent variable naming across the entire project:
- Input channels are fixed to 3 (RGB), assigned to `in_channels` and `in_channel`
- Output channels depend on the selected food category, and are assigned consistently to:
  `out_channels`, `out_channel`, `num_classes`, and `num_class`
"""

# === Set input channels ===
in_channels = in_channel = 3  # RGB images

# === Set output channels based on food class ===
if selected_category == "AdasPolo":
    out_channels = out_channel = num_classes = num_class = 2  # Background + AdasPolo
elif selected_category in ["CheloGoosht", "Fesenjan", "GheymeBademjan", "Protein_and_Fries"]:
    out_channels = out_channel = num_classes = num_class = 3  # Background + 2 foregrounds
else:
    raise ValueError(f"âŒ Unknown food category: {selected_category}")

# === Confirm final config ===
print("âœ… Configuration Set")
print(f"in_channels   = {in_channels}")
print(f"in_channel    = {in_channel} ")
print(f"out_channels  = {out_channels}")
print(f"out_channel   = {out_channel}")
print(f"num_classes   = {num_classes}")
print(f"num_class     = {num_class}")
print(f"ðŸ”Ž Category    = {selected_category}")

"""## **ðŸ§ MODELSðŸ§ **

### **ðŸ”·Original Unet [64,128,256,512,1024]**
"""

device = "cuda" if torch.cuda.is_available() else "cpu"
device

#                                                             PyTorch version
# =====================================================================================================================================================
# =====================================================================================================================================================
import torch
import torch.nn as nn

class UNet(nn.Module):

    # ==========================================================================
    def __init__(self, in_channels=None, out_channels=None):
        super(UNet, self).__init__()

        # =======================================
        # Original Unet proposed in => https://arxiv.org/pdf/1505.04597
        filters_model = [64, 128, 256, 512, 1024]
        # =======================================

        # Encoder                                                                                          # This is the down part, known as contracting path
        self.encoder1 = self.conv_block(in_channels, filters_model[0])                                     # Encoder 1 - should have 64 channels
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                                                 # Pooling 1

        self.encoder2 = self.conv_block(filters_model[0], filters_model[1])                                # Encoder 2 - should have 128 channels
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                                                 # Pooling 2

        self.encoder3 = self.conv_block(filters_model[1], filters_model[2])                                # Encoder 3 - should have 256 channels
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)                                                 # Pooling 3

        self.encoder4 = self.conv_block(filters_model[2], filters_model[3])                                # Encoder 4 - should have 512 channels
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)                                                 # Pooling 4

        # Bottleneck
        self.bottleneck = self.conv_block(filters_model[3], filters_model[4])                              # The original Unet => bottleneck should have 1024 channels

        # Decoder
        self.upconv4 = nn.ConvTranspose2d(filters_model[4], filters_model[3], kernel_size=2, stride=2)     # Upconv / TransposConv
        self.decoder4 = self.conv_block(filters_model[4], filters_model[3])                                # Decoder 4

        self.upconv3 = nn.ConvTranspose2d(filters_model[3], filters_model[2], kernel_size=2, stride=2)     # Upconv / TransposConv
        self.decoder3 = self.conv_block(filters_model[3], filters_model[2])                                # Decoder 3

        self.upconv2 = nn.ConvTranspose2d(filters_model[2], filters_model[1], kernel_size=2, stride=2)     # Upconv / TransposConv
        self.decoder2 = self.conv_block(filters_model[2], filters_model[1])                                # Decoder 2

        self.upconv1 = nn.ConvTranspose2d(filters_model[1], filters_model[0], kernel_size=2, stride=2)     # Upconv / TransposConv
        self.decoder1 = self.conv_block(filters_model[1], filters_model[0])                                # Decoder 1

        # Final Convolution
        self.conv = nn.Conv2d(filters_model[0], out_channels, kernel_size=1)                               # Final Conv (converting to out_channels)
    # ==========================================================================



    # Conv block
    # ==========================================================================
    def conv_block(self, in_channels, out_channels):                            # This "conv_block" is a base of the encoders/decoders written at the top
        return nn.Sequential(
                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                            nn.BatchNorm2d(out_channels),
                            nn.ReLU(inplace=True),
                            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                            nn.BatchNorm2d(out_channels),
                            nn.ReLU(inplace=True),
                            )                                                   # Can be used both as encoder and decoder
    # ==========================================================================




    # ==========================================================================
    def forward(self, x):
        # Encoder path
        enc1 = self.encoder1(x)
        enc1_pool = self.pool1(enc1)

        enc2 = self.encoder2(enc1_pool)
        enc2_pool = self.pool2(enc2)

        enc3 = self.encoder3(enc2_pool)
        enc3_pool = self.pool3(enc3)

        enc4 = self.encoder4(enc3_pool)
        enc4_pool = self.pool4(enc4)

        # Bottleneck
        bottleneck = self.bottleneck(enc4_pool)

        # Decoder path
        dec4 = self.upconv4(bottleneck)
        dec4_cat = torch.cat((dec4, enc4), dim=1)                               # Skip connection (dim=1 means concatenate them from channels)
        dec4_out = self.decoder4(dec4_cat)

        dec3 = self.upconv3(dec4_out)
        dec3_cat = torch.cat((dec3, enc3), dim=1)                               # Skip connection (dim=1 means concatenate them from channels)
        dec3_out = self.decoder3(dec3_cat)

        dec2 = self.upconv2(dec3_out)
        dec2_cat = torch.cat((dec2, enc2), dim=1)                               # Skip connection (dim=1 means concatenate them from channels)
        dec2_out = self.decoder2(dec2_cat)

        dec1 = self.upconv1(dec2_out)
        dec1_cat = torch.cat((dec1, enc1), dim=1)                               # Skip connection (dim=1 means concatenate them from channels)
        dec1_out = self.decoder1(dec1_cat)

        return self.conv(dec1_out)
    # ==========================================================================

if __name__ == "__main__":
    model = UNet(in_channels, out_channels)                                     # RGB input and binary output
# ==============================================================================
# Calculate total and trainable parameters
total_params     = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Convert to millions
total_params_millions = total_params         / 1_000_000
trainable_params_millions = trainable_params / 1_000_000

# Print results
print("================================================================")
print("================================================================")
print(f"Total Parameters:        {total_params_millions}        million")
print(f"Trainable Parameters:    {trainable_params_millions}    million")

# Checking in and out channels for consistency

print(f"in channels and out channels are {in_channels}, and {out_channels} respectively")

"""A simple check. We will make it comment later!"""

# # Example of how to create an instance of the U-Net model
# # ========================================================================================================
# if __name__ == "__main__":
#     model = UNet(in_channels=in_channels, out_channels=out_channels)            # RGB input and binary output => out_channels
#     # print(model)                                                                # Print the model architecture


# # Example input tensor
# # ========================================================================================================
# print(f"Image size is: {image_size}")
# print("")
# x_example = torch.randn(4, 3, image_size , image_size)                          # The first number is the Batch Size
# output_example = model(x_example)
# print(f"Output shape: {output_example.shape}")                                  # 4*out_channel*image_size*image_size is expected

"""### **ðŸ”·Smaller Unet  [32,64,128,256,512]**

Smaller Unet [32,64,128,256,512 instead of 64,128,256,512,1024]
"""

device = "cuda" if torch.cuda.is_available() else "cpu"
device

import torch
import torch.nn as nn

class SmallerUNet(nn.Module):
    def __init__(self, in_channels=None, out_channels=None):
        super(SmallerUNet, self).__init__()

        # Smaller U-Net architecture (halved filters from original U-Net)
        filters_model = [32, 64, 128, 256, 512]                                  # [32,64,128,256,512] instead of [64,128,526,512, and 1024]

        # Encoder (contracting path)
        self.encoder1 = self.conv_block(in_channels, filters_model[0])
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.encoder2 = self.conv_block(filters_model[0], filters_model[1])
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.encoder3 = self.conv_block(filters_model[1], filters_model[2])
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.encoder4 = self.conv_block(filters_model[2], filters_model[3])
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Bottleneck
        self.bottleneck = self.conv_block(filters_model[3], filters_model[4])

        # Decoder (expanding path)
        self.upconv4 = nn.ConvTranspose2d(filters_model[4], filters_model[3], kernel_size=2, stride=2)
        self.decoder4 = self.conv_block(filters_model[4], filters_model[3])

        self.upconv3 = nn.ConvTranspose2d(filters_model[3], filters_model[2], kernel_size=2, stride=2)
        self.decoder3 = self.conv_block(filters_model[3], filters_model[2])

        self.upconv2 = nn.ConvTranspose2d(filters_model[2], filters_model[1], kernel_size=2, stride=2)
        self.decoder2 = self.conv_block(filters_model[2], filters_model[1])

        self.upconv1 = nn.ConvTranspose2d(filters_model[1], filters_model[0], kernel_size=2, stride=2)
        self.decoder1 = self.conv_block(filters_model[1], filters_model[0])

        # Final classifier
        self.conv = nn.Conv2d(filters_model[0], out_channels, kernel_size=1)

    # Shared conv block for encoder/decoder
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        # Encoder
        enc1 = self.encoder1(x)
        enc1_pool = self.pool1(enc1)

        enc2 = self.encoder2(enc1_pool)
        enc2_pool = self.pool2(enc2)

        enc3 = self.encoder3(enc2_pool)
        enc3_pool = self.pool3(enc3)

        enc4 = self.encoder4(enc3_pool)
        enc4_pool = self.pool4(enc4)

        # Bottleneck
        bottleneck = self.bottleneck(enc4_pool)

        # Decoder
        dec4 = self.upconv4(bottleneck)
        dec4_cat = torch.cat((dec4, enc4), dim=1)
        dec4_out = self.decoder4(dec4_cat)

        dec3 = self.upconv3(dec4_out)
        dec3_cat = torch.cat((dec3, enc3), dim=1)
        dec3_out = self.decoder3(dec3_cat)

        dec2 = self.upconv2(dec3_out)
        dec2_cat = torch.cat((dec2, enc2), dim=1)
        dec2_out = self.decoder2(dec2_cat)

        dec1 = self.upconv1(dec2_out)
        dec1_cat = torch.cat((dec1, enc1), dim=1)
        dec1_out = self.decoder1(dec1_cat)

        return self.conv(dec1_out)

#                                 PyTorch version
# ==============================================================================

if __name__ == "__main__":
    model = SmallerUNet(in_channels, out_channels)                                     # RGB input and binary output
# ==============================================================================
# Calculate total and trainable parameters
total_params     = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Convert to millions
total_params_millions = total_params         / 1_000_000
trainable_params_millions = trainable_params / 1_000_000

# Print results
print("================================================================")
print("================================================================")
print(f"Total Parameters:        {total_params_millions}        million")
print(f"Trainable Parameters:    {trainable_params_millions}    million")

# Checking in and out channels for consistency

print(f"in channels and out channels are {in_channels}, and {out_channels} respectively")

"""The next cell is a simple check and will be made commment later!"""

# # Example of how to create an instance of the U-Net model
# # ========================================================================================================
# if __name__ == "__main__":
#     model = SmallerUNet(in_channels=in_channels, out_channels=out_channels)       # RGB input and binary output => out_channels
#     # print(model)                                                                # Print the model architecture


# # Example input tensor
# # ========================================================================================================
# print(f"Image size is: {image_size}")
# print("")
# x_example = torch.randn(4, 3, image_size , image_size)                          # The first number is the Batch Size
# output_example = model(x_example)
# print(f"Output shape: {output_example.shape}")                                  # 4*out_channel*image_size*image_size is expected

"""### **ðŸ”·NestNet [64,128,256,512,1024]**"""

device = "cuda" if torch.cuda.is_available() else "cpu"
device

#                         CLASS & Based on PyTorch (OK)
# ==============================================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the NestNet class
class NestNet(nn.Module):
    def __init__(self, in_channels=None, out_channels=None):

        super(NestNet, self).__init__()
        # Initialize parameters for the network
        self.in_channels  = in_channels
        self.out_channels = out_channels

        # Better version
        self.nb_filter = [64, 128, 256, 512, 1024]
        # LESS version
        # self.nb_filter = [32, 64, 128, 256, 512]

        # Define layers
        # Contracting path layers
        self.conv1_1 = self.standard_pool_unit(self.in_channels, self.nb_filter[0])
        self.conv2_1 = self.standard_pool_unit(self.nb_filter[0], self.nb_filter[1])
        self.conv3_1 = self.standard_pool_unit(self.nb_filter[1], self.nb_filter[2])
        self.conv4_1 = self.standard_pool_unit(self.nb_filter[2], self.nb_filter[3])
        self.conv5_1 = self.standard_unit(self.nb_filter[3], self.nb_filter[4])

        # Nested blocks
        self.conv1_2 = self.standard_unit(self.nb_filter[0] * 2, self.nb_filter[0])
        self.conv2_2 = self.standard_unit(self.nb_filter[1] * 2, self.nb_filter[1])
        self.conv1_3 = self.standard_unit(self.nb_filter[0] * 3, self.nb_filter[0])
        self.conv3_2 = self.standard_unit(self.nb_filter[2] * 2, self.nb_filter[2])
        self.conv2_3 = self.standard_unit(self.nb_filter[1] * 3, self.nb_filter[1])
        self.conv1_4 = self.standard_unit(self.nb_filter[0] * 4, self.nb_filter[0])
        self.conv4_2 = self.standard_unit(self.nb_filter[3] * 2, self.nb_filter[3])
        self.conv3_3 = self.standard_unit(self.nb_filter[2] * 3, self.nb_filter[2])
        self.conv2_4 = self.standard_unit(self.nb_filter[1] * 4, self.nb_filter[1])
        self.conv1_5 = self.standard_unit(self.nb_filter[0] * 5, self.nb_filter[0])

        # Transpose convolution (upsampling)
        self.up1_2 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)
        self.up1_3 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)
        self.up2_2 = nn.ConvTranspose2d(self.nb_filter[2], self.nb_filter[1], kernel_size=2, stride=2)
        self.up2_3 = nn.ConvTranspose2d(self.nb_filter[2], self.nb_filter[1], kernel_size=2, stride=2)
        self.up2_4 = nn.ConvTranspose2d(self.nb_filter[2], self.nb_filter[1], kernel_size=2, stride=2)
        self.up3_2 = nn.ConvTranspose2d(self.nb_filter[3], self.nb_filter[2], kernel_size=2, stride=2)
        self.up3_3 = nn.ConvTranspose2d(self.nb_filter[3], self.nb_filter[2], kernel_size=2, stride=2)
        self.up4_2 = nn.ConvTranspose2d(self.nb_filter[4], self.nb_filter[3], kernel_size=2, stride=2)
        self.up1_4 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)
        self.up1_5 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)

        # Output layer
        self.output_layer = nn.Conv2d(self.nb_filter[0], self.out_channels, kernel_size=1, padding=0)

    # ===============================================================================================
    def standard_pool_unit(self, in_channels, out_channels, kernel_size=3):
        act = nn.ReLU(inplace=True)
        layers = [
                  nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
                  nn.BatchNorm2d(out_channels),
                  act,
                  nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
                  nn.BatchNorm2d(out_channels),
                  act
                  ]
        return nn.Sequential(*layers)

    def standard_unit(self, in_channels, out_channels, kernel_size=3):
        act = nn.ReLU(inplace=True)
        layers = [
                  nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
                  act,
                  nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
                  act
                  ]
        return nn.Sequential(*layers)
    # ================================================================================================


    def forward(self, x):
        # Contracting path (going down) + Creating nested blocks
        # ======================================================
        # Block 1*1
        conv1_1 = self.conv1_1(x)
        pool1 = F.max_pool2d(conv1_1, kernel_size=2)

        # Block 2*1
        conv2_1 = self.conv2_1(pool1)
        pool2 = F.max_pool2d(conv2_1, kernel_size=2)

        # Block 1*2
        up1_2 = self.up1_2(conv2_1)
        conv1_2 = torch.cat([up1_2, conv1_1], dim=1)
        conv1_2 = self.conv1_2(conv1_2)

        # Block 3*1
        conv3_1 = self.conv3_1(pool2)
        pool3 = F.max_pool2d(conv3_1, kernel_size=2)

        # Block 2*2
        up2_2 = self.up2_2(conv3_1)
        conv2_2 = torch.cat([up2_2, conv2_1], dim=1)
        conv2_2 = self.conv2_2(conv2_2)

        # Block 1*3
        up1_3 = self.up1_3(conv2_2)
        conv1_3 = torch.cat([up1_3, conv1_1, conv1_2], dim=1)
        conv1_3 = self.conv1_3(conv1_3)

        # Block 4*1
        conv4_1 = self.conv4_1(pool3)
        drop4 = F.dropout(conv4_1, p=0.5, training=self.training)
        pool4 = F.max_pool2d(drop4, kernel_size=2)

        # Block 3*2
        up3_2 = self.up3_2(conv4_1)
        conv3_2 = torch.cat([up3_2, conv3_1], dim=1)
        conv3_2 = self.conv3_2(conv3_2)

        # Block 2*3
        up2_3 = self.up2_3(conv3_2)
        conv2_3 = torch.cat([up2_3, conv2_1, conv2_2], dim=1)
        conv2_3 = self.conv2_3(conv2_3)

        # Block 1*4
        up1_4 = self.up1_4(conv2_3)
        conv1_4 = torch.cat([up1_4, conv1_1, conv1_2, conv1_3], dim=1)
        conv1_4 = self.conv1_4(conv1_4)

        # Block 5*1 => Bottleneck
        conv5_1 = self.conv5_1(pool4)
        conv5_1 = F.dropout(conv5_1, p=0.5, training=self.training)

        # Upconv / Decoder / going up + Nested blocks
        # ======================================================
        # Block 4*2
        up4_2 = self.up4_2(conv5_1)
        conv4_2 = torch.cat([up4_2, conv4_1], dim=1)
        conv4_2 = self.conv4_2(conv4_2)

        # Block 3*3
        up3_3 = self.up3_3(conv4_2)
        conv3_3 = torch.cat([up3_3, conv3_1, conv3_2], dim=1)
        conv3_3 = self.conv3_3(conv3_3)

        # Block 2*4
        up2_4 = self.up2_4(conv3_3)
        conv2_4 = torch.cat([up2_4, conv2_1, conv2_2, conv2_3], dim=1)
        conv2_4 = self.conv2_4(conv2_4)

        # Block 1*5
        up1_5 = self.up1_5(conv2_4)
        conv1_5 = torch.cat([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], dim=1)
        conv1_5 = self.conv1_5(conv1_5)

        # Output layer
        output = self.output_layer(conv1_5)
        return output

#                                 PyTorch version
# ==============================================================================

if __name__ == "__main__":
    model = NestNet(in_channels, out_channels)                                     # RGB input and binary output
# ==============================================================================
# Calculate total and trainable parameters
total_params     = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Convert to millions
total_params_millions = total_params         / 1_000_000
trainable_params_millions = trainable_params / 1_000_000

# Print results
print("================================================================")
print("================================================================")
print(f"Total Parameters:        {total_params_millions}        million")
print(f"Trainable Parameters:    {trainable_params_millions}    million")

# Checking in and out channels for consistency

print(f"in channels and out channels are {in_channels}, and {out_channels} respectively")

"""The next cell is an in-depth check section of the NestNet model. Please make it comment after checking that all is right"""

# # The follwing code is only and only designed for an in-depth check. If you run that, comment it again, then run the notebook from the ground up


# # Assuming the class definition for NestNet is already defined and imported as per your code above

# # Create a sample input tensor
# image_like_tensor_for_example = torch.randn(1, 3, 64, 64)  # [batch_size, channels, height, width]

# # Instantiate the model
# model = NestNet(in_channels=in_channels, out_channels=out_channels)  # Adjust output channels as needed

# # Update the forward function to include shape printing
# def modified_forward(self, x):
#     # Contracting path (going down) + Creating nested blocks
#     # ======================================================
#     print("Input shape:", x.shape)
#     print("")
#     # Block 1*1
#     conv1_1 = self.conv1_1(x)
#     print("After conv1_1:", conv1_1.shape)
#     pool1 = F.max_pool2d(conv1_1, kernel_size=2)
#     print("After pool1:", pool1.shape)
#     print("")
#     # Block 2*1
#     conv2_1 = self.conv2_1(pool1)
#     print("After conv2_1:", conv2_1.shape)
#     pool2 = F.max_pool2d(conv2_1, kernel_size=2)
#     print("After pool2:", pool2.shape)
#     print("")
#     # Block 1*2
#     up1_2 = self.up1_2(conv2_1)
#     print("After up1_2:", up1_2.shape)
#     conv1_2 = torch.cat([up1_2, conv1_1], dim=1)
#     print("After concat conv1_2:", conv1_2.shape)
#     conv1_2 = self.conv1_2(conv1_2)
#     print("After conv1_2:", conv1_2.shape)
#     print("")
#     # Block 3*1
#     conv3_1 = self.conv3_1(pool2)
#     print("After conv3_1:", conv3_1.shape)
#     pool3 = F.max_pool2d(conv3_1, kernel_size=2)
#     print("After pool3:", pool3.shape)
#     print("")
#     # Block 2*2
#     up2_2 = self.up2_2(conv3_1)
#     print("After up2_2:", up2_2.shape)
#     conv2_2 = torch.cat([up2_2, conv2_1], dim=1)
#     print("After concat conv2_2:", conv2_2.shape)
#     conv2_2 = self.conv2_2(conv2_2)
#     print("After conv2_2:", conv2_2.shape)
#     print("")
#     # Block 1*3
#     up1_3 = self.up1_3(conv2_2)
#     print("After up1_3:", up1_3.shape)
#     conv1_3 = torch.cat([up1_3, conv1_1, conv1_2], dim=1)
#     print("After concat conv1_3:", conv1_3.shape)
#     conv1_3 = self.conv1_3(conv1_3)
#     print("After conv1_3:", conv1_3.shape)
#     print("")
#     # Block 4*1
#     conv4_1 = self.conv4_1(pool3)
#     print("After conv4_1:", conv4_1.shape)
#     drop4 = F.dropout(conv4_1, p=0.5, training=self.training)
#     print("After dropout drop4:", drop4.shape)
#     pool4 = F.max_pool2d(drop4, kernel_size=2)
#     print("After pool4:", pool4.shape)
#     print("")
#     # Block 3*2
#     up3_2 = self.up3_2(conv4_1)
#     print("After up3_2:", up3_2.shape)
#     conv3_2 = torch.cat([up3_2, conv3_1], dim=1)
#     print("After concat conv3_2:", conv3_2.shape)
#     conv3_2 = self.conv3_2(conv3_2)
#     print("After conv3_2:", conv3_2.shape)
#     print("")
#     # Block 2*3
#     up2_3 = self.up2_3(conv3_2)
#     print("After up2_3:", up2_3.shape)
#     conv2_3 = torch.cat([up2_3, conv2_1, conv2_2], dim=1)
#     print("After concat conv2_3:", conv2_3.shape)
#     conv2_3 = self.conv2_3(conv2_3)
#     print("After conv2_3:", conv2_3.shape)
#     print("")
#     # Block 1*4
#     up1_4 = self.up1_4(conv2_3)
#     print("After up1_4:", up1_4.shape)
#     conv1_4 = torch.cat([up1_4, conv1_1, conv1_2, conv1_3], dim=1)
#     print("After concat conv1_4:", conv1_4.shape)
#     conv1_4 = self.conv1_4(conv1_4)
#     print("After conv1_4:", conv1_4.shape)
#     print("")
#     # Block 5*1 => Bottleneck
#     conv5_1 = self.conv5_1(pool4)
#     print("After conv5_1:", conv5_1.shape)
#     conv5_1 = F.dropout(conv5_1, p=0.5, training=self.training)
#     print("After dropout conv5_1:", conv5_1.shape)
#     print("")
#     # Upconv / Decoder / going up + Nested blocks
#     # ======================================================
#     # Block 4*2
#     up4_2 = self.up4_2(conv5_1)
#     print("After up4_2:", up4_2.shape)
#     conv4_2 = torch.cat([up4_2, conv4_1], dim=1)
#     print("After concat conv4_2:", conv4_2.shape)
#     conv4_2 = self.conv4_2(conv4_2)
#     print("After conv4_2:", conv4_2.shape)
#     print("")
#     # Block 3*3
#     up3_3 = self.up3_3(conv4_2)
#     print("After up3_3:", up3_3.shape)
#     conv3_3 = torch.cat([up3_3, conv3_1, conv3_2], dim=1)
#     print("After concat conv3_3:", conv3_3.shape)
#     conv3_3 = self.conv3_3(conv3_3)
#     print("After conv3_3:", conv3_3.shape)
#     print("")
#     # Block 2*4
#     up2_4 = self.up2_4(conv3_3)
#     print("After up2_4:", up2_4.shape)
#     conv2_4 = torch.cat([up2_4, conv2_1, conv2_2, conv2_3], dim=1)
#     print("After concat conv2_4:", conv2_4.shape)
#     conv2_4 = self.conv2_4(conv2_4)
#     print("After conv2_4:", conv2_4.shape)
#     print("")
#     # Block 1*5
#     up1_5 = self.up1_5(conv2_4)
#     print("After up1_5:", up1_5.shape)
#     conv1_5 = torch.cat([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], dim=1)
#     print("After concat conv1_5:", conv1_5.shape)
#     conv1_5 = self.conv1_5(conv1_5)
#     print("After conv1_5:", conv1_5.shape)
#     print("")
#     # Output layer
#     output = self.output_layer(conv1_5)
#     print("After output layer:", output.shape)
#     return output
#     print("")

# # Bind the modified forward function to our model
# model.forward = modified_forward.__get__(model, NestNet)

# # Run the model with the sample input
# output = model(image_like_tensor_for_example)

"""### **ðŸ”·Smaller NestNet [32,64,128,256,512]**"""

device = "cuda" if torch.cuda.is_available() else "cpu"
device

import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the Smaller_NestNet class (Lightweight U-Net++)
class Smaller_NestNet(nn.Module):
    def __init__(self, in_channels=None, out_channels=None):
        super(Smaller_NestNet, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels

        # Lightweight version with fewer filters
        self.nb_filter = [32, 64, 128, 256, 512]

        # Contracting path (encoder)
        self.conv1_1 = self.standard_pool_unit(self.in_channels, self.nb_filter[0])
        self.conv2_1 = self.standard_pool_unit(self.nb_filter[0], self.nb_filter[1])
        self.conv3_1 = self.standard_pool_unit(self.nb_filter[1], self.nb_filter[2])
        self.conv4_1 = self.standard_pool_unit(self.nb_filter[2], self.nb_filter[3])
        self.conv5_1 = self.standard_unit(self.nb_filter[3], self.nb_filter[4])  # bottleneck

        # Nested decoder blocks
        self.conv1_2 = self.standard_unit(self.nb_filter[0] * 2, self.nb_filter[0])
        self.conv2_2 = self.standard_unit(self.nb_filter[1] * 2, self.nb_filter[1])
        self.conv1_3 = self.standard_unit(self.nb_filter[0] * 3, self.nb_filter[0])
        self.conv3_2 = self.standard_unit(self.nb_filter[2] * 2, self.nb_filter[2])
        self.conv2_3 = self.standard_unit(self.nb_filter[1] * 3, self.nb_filter[1])
        self.conv1_4 = self.standard_unit(self.nb_filter[0] * 4, self.nb_filter[0])
        self.conv4_2 = self.standard_unit(self.nb_filter[3] * 2, self.nb_filter[3])
        self.conv3_3 = self.standard_unit(self.nb_filter[2] * 3, self.nb_filter[2])
        self.conv2_4 = self.standard_unit(self.nb_filter[1] * 4, self.nb_filter[1])
        self.conv1_5 = self.standard_unit(self.nb_filter[0] * 5, self.nb_filter[0])

        # Upsampling blocks
        self.up1_2 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)
        self.up1_3 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)
        self.up1_4 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)
        self.up1_5 = nn.ConvTranspose2d(self.nb_filter[1], self.nb_filter[0], kernel_size=2, stride=2)

        self.up2_2 = nn.ConvTranspose2d(self.nb_filter[2], self.nb_filter[1], kernel_size=2, stride=2)
        self.up2_3 = nn.ConvTranspose2d(self.nb_filter[2], self.nb_filter[1], kernel_size=2, stride=2)
        self.up2_4 = nn.ConvTranspose2d(self.nb_filter[2], self.nb_filter[1], kernel_size=2, stride=2)

        self.up3_2 = nn.ConvTranspose2d(self.nb_filter[3], self.nb_filter[2], kernel_size=2, stride=2)
        self.up3_3 = nn.ConvTranspose2d(self.nb_filter[3], self.nb_filter[2], kernel_size=2, stride=2)

        self.up4_2 = nn.ConvTranspose2d(self.nb_filter[4], self.nb_filter[3], kernel_size=2, stride=2)

        # Final output layer
        self.output_layer = nn.Conv2d(self.nb_filter[0], self.out_channels, kernel_size=1, padding=0)

    # Standard encoder block
    def standard_pool_unit(self, in_channels, out_channels, kernel_size=3):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    # Standard decoder block
    def standard_unit(self, in_channels, out_channels, kernel_size=3):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # Encoder
        conv1_1 = self.conv1_1(x)
        pool1 = F.max_pool2d(conv1_1, kernel_size=2)

        conv2_1 = self.conv2_1(pool1)
        pool2 = F.max_pool2d(conv2_1, kernel_size=2)

        up1_2 = self.up1_2(conv2_1)
        conv1_2 = self.conv1_2(torch.cat([up1_2, conv1_1], dim=1))

        conv3_1 = self.conv3_1(pool2)
        pool3 = F.max_pool2d(conv3_1, kernel_size=2)

        up2_2 = self.up2_2(conv3_1)
        conv2_2 = self.conv2_2(torch.cat([up2_2, conv2_1], dim=1))

        up1_3 = self.up1_3(conv2_2)
        conv1_3 = self.conv1_3(torch.cat([up1_3, conv1_1, conv1_2], dim=1))

        conv4_1 = self.conv4_1(pool3)
        drop4 = F.dropout(conv4_1, p=0.5, training=self.training)
        pool4 = F.max_pool2d(drop4, kernel_size=2)

        up3_2 = self.up3_2(conv4_1)
        conv3_2 = self.conv3_2(torch.cat([up3_2, conv3_1], dim=1))

        up2_3 = self.up2_3(conv3_2)
        conv2_3 = self.conv2_3(torch.cat([up2_3, conv2_1, conv2_2], dim=1))

        up1_4 = self.up1_4(conv2_3)
        conv1_4 = self.conv1_4(torch.cat([up1_4, conv1_1, conv1_2, conv1_3], dim=1))

        conv5_1 = self.conv5_1(pool4)
        conv5_1 = F.dropout(conv5_1, p=0.5, training=self.training)

        up4_2 = self.up4_2(conv5_1)
        conv4_2 = self.conv4_2(torch.cat([up4_2, conv4_1], dim=1))

        up3_3 = self.up3_3(conv4_2)
        conv3_3 = self.conv3_3(torch.cat([up3_3, conv3_1, conv3_2], dim=1))

        up2_4 = self.up2_4(conv3_3)
        conv2_4 = self.conv2_4(torch.cat([up2_4, conv2_1, conv2_2, conv2_3], dim=1))

        up1_5 = self.up1_5(conv2_4)
        conv1_5 = self.conv1_5(torch.cat([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], dim=1))

        return self.output_layer(conv1_5)

#                                 PyTorch version
# ==============================================================================

if __name__ == "__main__":
    model = Smaller_NestNet(in_channels, out_channels)                                     # RGB input and binary output
# ==============================================================================
# Calculate total and trainable parameters
total_params     = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Convert to millions
total_params_millions = total_params         / 1_000_000
trainable_params_millions = trainable_params / 1_000_000

# Print results
print("================================================================")
print("================================================================")
print(f"Total Parameters:        {total_params_millions}        million")
print(f"Trainable Parameters:    {trainable_params_millions}    million")

# Checking in and out channels for consistency

print(f"in channels and out channels are {in_channels}, and {out_channels} respectively")

"""The next cell is an in-depth check of Smaller Unet. Make it comment after running"""

# import torch
# import torch.nn.functional as F

# # Parameters
# in_channels = 3
# out_channels = 15

# # Dummy input
# image_like_tensor_for_example = torch.randn(1, in_channels, 64, 64)

# # Instantiate the model
# model = Smaller_NestNet(in_channels=in_channels, out_channels=out_channels)

# # Modified forward function to inspect shapes
# def modified_forward(self, x):
#     print("Input shape:", x.shape, "\n")

#     conv1_1 = self.conv1_1(x)
#     print("conv1_1:", conv1_1.shape)
#     pool1 = F.max_pool2d(conv1_1, 2)
#     print("pool1:", pool1.shape, "\n")

#     conv2_1 = self.conv2_1(pool1)
#     print("conv2_1:", conv2_1.shape)
#     pool2 = F.max_pool2d(conv2_1, 2)
#     print("pool2:", pool2.shape, "\n")

#     up1_2 = self.up1_2(conv2_1)
#     print("up1_2:", up1_2.shape)
#     conv1_2 = self.conv1_2(torch.cat([up1_2, conv1_1], dim=1))
#     print("conv1_2:", conv1_2.shape, "\n")

#     conv3_1 = self.conv3_1(pool2)
#     print("conv3_1:", conv3_1.shape)
#     pool3 = F.max_pool2d(conv3_1, 2)
#     print("pool3:", pool3.shape, "\n")

#     up2_2 = self.up2_2(conv3_1)
#     print("up2_2:", up2_2.shape)
#     conv2_2 = self.conv2_2(torch.cat([up2_2, conv2_1], dim=1))
#     print("conv2_2:", conv2_2.shape, "\n")

#     up1_3 = self.up1_3(conv2_2)
#     print("up1_3:", up1_3.shape)
#     conv1_3 = self.conv1_3(torch.cat([up1_3, conv1_1, conv1_2], dim=1))
#     print("conv1_3:", conv1_3.shape, "\n")

#     conv4_1 = self.conv4_1(pool3)
#     print("conv4_1:", conv4_1.shape)
#     drop4 = F.dropout(conv4_1, p=0.5, training=self.training)
#     print("drop4:", drop4.shape)
#     pool4 = F.max_pool2d(drop4, 2)
#     print("pool4:", pool4.shape, "\n")

#     up3_2 = self.up3_2(conv4_1)
#     print("up3_2:", up3_2.shape)
#     conv3_2 = self.conv3_2(torch.cat([up3_2, conv3_1], dim=1))
#     print("conv3_2:", conv3_2.shape, "\n")

#     up2_3 = self.up2_3(conv3_2)
#     print("up2_3:", up2_3.shape)
#     conv2_3 = self.conv2_3(torch.cat([up2_3, conv2_1, conv2_2], dim=1))
#     print("conv2_3:", conv2_3.shape, "\n")

#     up1_4 = self.up1_4(conv2_3)
#     print("up1_4:", up1_4.shape)
#     conv1_4 = self.conv1_4(torch.cat([up1_4, conv1_1, conv1_2, conv1_3], dim=1))
#     print("conv1_4:", conv1_4.shape, "\n")

#     conv5_1 = self.conv5_1(pool4)
#     print("conv5_1:", conv5_1.shape)
#     conv5_1 = F.dropout(conv5_1, p=0.5, training=self.training)
#     print("dropout conv5_1:", conv5_1.shape, "\n")

#     up4_2 = self.up4_2(conv5_1)
#     print("up4_2:", up4_2.shape)
#     conv4_2 = self.conv4_2(torch.cat([up4_2, conv4_1], dim=1))
#     print("conv4_2:", conv4_2.shape, "\n")

#     up3_3 = self.up3_3(conv4_2)
#     print("up3_3:", up3_3.shape)
#     conv3_3 = self.conv3_3(torch.cat([up3_3, conv3_1, conv3_2], dim=1))
#     print("conv3_3:", conv3_3.shape, "\n")

#     up2_4 = self.up2_4(conv3_3)
#     print("up2_4:", up2_4.shape)
#     conv2_4 = self.conv2_4(torch.cat([up2_4, conv2_1, conv2_2, conv2_3], dim=1))
#     print("conv2_4:", conv2_4.shape, "\n")

#     up1_5 = self.up1_5(conv2_4)
#     print("up1_5:", up1_5.shape)
#     conv1_5 = self.conv1_5(torch.cat([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], dim=1))
#     print("conv1_5:", conv1_5.shape, "\n")

#     output = self.output_layer(conv1_5)
#     print("ðŸŽ¯ Final Output Shape:", output.shape)
#     return output

# # Bind modified forward for inspection
# model.forward = modified_forward.__get__(model, Smaller_NestNet)

# # Run it
# _ = model(image_like_tensor_for_example)


# _ = model(image_like_tensor_for_example)

# # â†“ Add this after running the model
# print("="*60)
# total_params = sum(p.numel() for p in model.parameters())
# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
# print("ðŸ§  Total Parameters:     ", total_params)
# print("ðŸ› ï¸ Trainable Parameters:", trainable_params)

"""## âœ…ðŸ§ PICK Your MODEL here, more CHECK

These numbers may be approxiamte

![model summary.png](data:image/png;base64,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)
"""

import torch

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters()) / 1e6
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6

# Display counts in millions
print(f"ðŸ§  Total Parameters:     {total_params:.2f}M")
print(f"ðŸ› ï¸ Trainable Parameters: {trainable_params:.2f}M")

"""## **âš–ï¸LOSS Function(s)**

More Weights for Larger Classes (Not recommended at all as lead to smaller classes to be neglected)
"""

# """
# This module defines DynamicWeightedCrossEntropy, a custom loss function for
# multi-class semantic segmentation with class imbalance. It computes per-batch
# class weights based on the current ground truth, giving more importance to
# classes with more pixels (e.g., background) to reflect their prevalence in the batch.
# """

# import torch
# import torch.nn as nn
# import torch.nn.functional as F

# class DynamicWeightedCrossEntropy(nn.Module):
#     def __init__(self, num_classes, eps=1e-7):
#         """
#         Args:
#             num_classes (int): Number of output classes (including background)
#             eps (float): Small value to avoid division by zero
#         """
#         super(DynamicWeightedCrossEntropy, self).__init__()
#         self.num_classes = num_classes
#         self.eps = eps

#     def forward(self, preds, targets):
#         """
#         Args:
#             preds (Tensor): Predicted logits of shape [B, C, H, W]
#             targets (Tensor): Ground truth labels of shape [B, H, W] with values in [0, C-1]
#         """
#         B, C, H, W = preds.shape
#         preds = preds.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, C]
#         targets = targets.view(-1)                        # [B*H*W]

#         # Count class occurrences in batch
#         pixel_counts = torch.bincount(targets, minlength=self.num_classes).float()
#         total_pixels = pixel_counts.sum() + self.eps
#         class_weights = pixel_counts / total_pixels       # Normalize to [0, 1]
#         class_weights = class_weights.clamp(min=self.eps)  # Avoid zero weights

#         # Weighted Negative Log-Likelihood loss
#         log_probs = F.log_softmax(preds, dim=1)
#         return F.nll_loss(log_probs, targets, weight=class_weights, reduction='mean')

# loss_function = DynamicWeightedCrossEntropy(num_classes=out_channels)

"""Normal Cross Entropy Loss (Not optimal as it treats both small and largers classes the same)"""

# loss_function = nn.CrossEntropyLoss()

"""More Weight for Smaller Classes (Reversed Pixel Weights) [This is good but can explode loss for very small classes, so goo but not optimal!]"""

# class DynamicWeightedCELoss(nn.Module):
#     def __init__(self, num_classes):
#         super().__init__()
#         self.num_classes = num_classes

#     def forward(self, inputs, targets):
#         N, C, H, W = inputs.shape
#         with torch.no_grad():
#             one_hot = F.one_hot(targets, num_classes=self.num_classes)
#             one_hot = one_hot.permute(0, 3, 1, 2).float()
#             class_counts = one_hot.sum(dim=(0, 2, 3))
#             total = class_counts.sum()
#             freq = class_counts / total
#             weights = 1.0 / (freq + 1e-6)
#             weights = weights / weights.sum() * self.num_classes

#         inputs = inputs.permute(0, 2, 3, 1).reshape(-1, C)
#         targets = targets.view(-1)
#         loss = F.cross_entropy(inputs, targets, weight=weights)
#         return loss

# loss_function = DynamicWeightedCELoss(num_classes=num_classes)

"""More Weights for Smaller Classes but Capped (Reversed Pixel Weights but No One Bigger than 10) [This is optimal from our point of view]"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class CappedDynamicWeightedCELoss(nn.Module):
    def __init__(self, num_classes, max_ratio=10.0):    # max ratio is here
        super().__init__()
        self.num_classes = num_classes
        self.max_ratio = max_ratio  # Ø­Ø¯Ø§Ú©Ø«Ø± Ù†Ø³Ø¨Øª ÙˆØ²Ù† Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ø¨Ù‡ Ú©ÙˆÚ†Ú©â€ŒØªØ±ÛŒÙ†

    def forward(self, inputs, targets):
        N, C, H, W = inputs.shape

        with torch.no_grad():
            # Step 1: one-hot encoding of targets
            one_hot = F.one_hot(targets, num_classes=self.num_classes)
            one_hot = one_hot.permute(0, 3, 1, 2).float()

            # Step 2: compute class frequencies
            class_counts = one_hot.sum(dim=(0, 2, 3))  # [C]
            total = class_counts.sum()
            freq = class_counts / (total + 1e-6)  # [C]
            raw_weights = 1.0 / (freq + 1e-6)

            # Step 3: apply max_ratio cap
            min_w = raw_weights.min()
            max_allowed = min_w * self.max_ratio
            capped_weights = torch.clamp(raw_weights, max=max_allowed)

            # Step 4: normalize weights
            weights = capped_weights / capped_weights.sum() * self.num_classes  # ensure sum ~= C

        # Step 5: reshape for cross entropy
        inputs = inputs.permute(0, 2, 3, 1).reshape(-1, C)
        targets = targets.view(-1)
        loss = F.cross_entropy(inputs, targets, weight=weights)
        return loss

loss_function = CappedDynamicWeightedCELoss(num_classes=num_classes)

"""## **ðŸ”§Optimizer + LR rate**"""

import torch
from torch.optim import AdamW

lr = 1e-3
wd = 1e-4

# The lr mentioned in here is for the initial epoch, and we may reduce it during different epochs using learning rate scheduler.

# Create AdamW optimizer
optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)

"""## **ðŸ“‰LR scheduler**"""

print(f"Your intial LR is ===> {lr}")

from torch.optim.lr_scheduler import LambdaLR

# Updated custom LR multiplier function
def custom_lr_schedule(epoch: int) -> float:
    """
    Learning rate multipliers based on epoch ranges for base lr = 1e-3:
    - Epochs 0â€“4    â†’ lr = 1e-3   (multiplier = 1.0)
    - Epochs 5â€“9    â†’ lr = 5e-4   (multiplier = 0.5)
    - Epochs 10â€“14  â†’ lr = 1e-4   (multiplier = 0.1)
    - Epochs 15â€“19  â†’ lr = 5e-5   (multiplier = 0.05)
    - Epochs 20+    â†’ lr = 1e-5   (multiplier = 0.01)
    """
    if epoch < 5:
        return 1.0
    elif epoch < 10:
        return 0.5
    elif epoch < 15:
        return 0.1
    elif epoch < 20:
        return 0.05
    else:
        return 0.01

# Attach LambdaLR scheduler to the optimizer
scheduler = LambdaLR(optimizer, lr_lambda=custom_lr_schedule)

## By 0-4, we mean 1-5.
# Epcoh 0 is No traning
# 1-5 => 1e-3, ..., 20-25 => 1e-5

# import matplotlib.pyplot as plt

# # Set global font to serif for an academic look
# plt.rcParams['font.family'] = 'serif'

# # Custom LR schedule (epochs start from 1)
# def custom_lr_schedule(epoch: int) -> float:
#     """
#     Learning rate multipliers based on epoch ranges for base lr = 1e-3:
#     - Epochs 1â€“5    â†’ lr = 1e-3   (multiplier = 1.0)
#     - Epochs 6â€“10   â†’ lr = 5e-4   (multiplier = 0.5)
#     - Epochs 11â€“15  â†’ lr = 1e-4   (multiplier = 0.1)
#     - Epochs 16â€“20  â†’ lr = 5e-5   (multiplier = 0.05)
#     - Epochs 21+    â†’ lr = 1e-5   (multiplier = 0.01)
#     """
#     if epoch <= 5:
#         return 1.0
#     elif epoch <= 10:
#         return 0.5
#     elif epoch <= 15:
#         return 0.1
#     elif epoch <= 20:
#         return 0.05
#     else:
#         return 0.01

# # Parameters
# base_lr = 1e-3
# total_epochs = 25

# # Compute scaled learning rate values
# multipliers = [custom_lr_schedule(epoch) for epoch in range(1, total_epochs + 1)]

# # Plot
# plt.figure(figsize=(8, 5))
# plt.plot(
#     range(1, total_epochs + 1),
#     multipliers,
#     marker='o',
#     linestyle='-',
#     color='slategray',
#     linewidth=2
# )

# plt.xlabel('Epoch', fontsize=12)
# plt.ylabel('Learning Rate (Ã—1e-3)', fontsize=12)
# plt.xticks([1, 5, 10, 15, 20, 25], fontsize=10)
# plt.yticks([1.0, 0.5, 0.1, 0.05, 0.01], labels=['1.0', '0.5', '0.1', '0.05', '0.01'], fontsize=10)
# plt.grid(True, linestyle='--', alpha=0.5)
# plt.tight_layout()

# # Save the figure
# plt.savefig("LR_Settings.png", dpi=800)
# plt.show()

"""## **ðŸ“Š METRICs**"""

!pip install -U torchmetrics

"""
Cell 16: Define evaluation metrics for multi-class semantic segmentation

Includes:
- Pixel Accuracy (macro, weighted)
- IoU (macro, weighted)
- Dice (macro, weighted, manually computed)

Works with dynamic number of classes (out_channels)
"""

import torch
from torchmetrics import Accuracy, JaccardIndex  # PyTorch metrics

# === Config ===
num_classes = out_channels  # dynamically set earlier
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"\nðŸ“Š Metric Setup: in_channels = {in_channels}, out_channels = {out_channels}, num_classes = {num_classes}\n")

# === Pixel Accuracy ===
pixel_accuracy_macro    = Accuracy(num_classes=num_classes, average='macro', task="multiclass").to(device)
pixel_accuracy_weighted = Accuracy(num_classes=num_classes, average='weighted', task="multiclass").to(device)

# === Intersection over Union (IoU) ===
iou_macro    = JaccardIndex(num_classes=num_classes, average='macro', task="multiclass").to(device)
iou_weighted = JaccardIndex(num_classes=num_classes, average='weighted', task="multiclass").to(device)

# === Dice Coefficient ===

# Macro Dice: Equal contribution per class
def dice_macro(preds, targets, num_classes):
    dice_scores = torch.zeros(num_classes, device=preds.device)
    for class_idx in range(num_classes):
        class_preds = (preds == class_idx).float()
        class_targets = (targets == class_idx).float()
        intersection = (class_preds * class_targets).sum()
        union = class_preds.sum() + class_targets.sum()
        dice_scores[class_idx] = (2 * intersection) / (union + 1e-6)
    return dice_scores.mean()

# Weighted Dice: Weighted by pixel count per class
def dice_weighted(preds, targets, num_classes):
    dice_scores = torch.zeros(num_classes, device=preds.device)
    pixel_counts = torch.zeros(num_classes, device=preds.device)
    for class_idx in range(num_classes):
        class_preds = (preds == class_idx).float()
        class_targets = (targets == class_idx).float()
        intersection = (class_preds * class_targets).sum()
        union = class_preds.sum() + class_targets.sum()
        dice_scores[class_idx] = (2 * intersection) / (union + 1e-6)
        pixel_counts[class_idx] = class_targets.sum()
    total_pixels = pixel_counts.sum()
    return (dice_scores * pixel_counts / (total_pixels + 1e-6)).sum()


print(f"\nðŸ“Š metrcis defined!")

"""
=================================================================================================================
ðŸ“Œ Distributional Pixel Agreement (DPA) â€“ Custom Metric for Food Waste Estimation based on Semantic Segmentation
=================================================================================================================

Author: Shayan Rokhva

Definition:
------------
Distributional Pixel Agreement (DPA) is a novel evaluation metric designed for
semantic segmentation tasks where **accurate estimation of class proportions** is
more critical than spatial accuracy. Unlike traditional metrics like pixel accuracy,
IoU, or Dice, which evaluate correctness based on exact pixel-wise locations, DPA
measures how well the **distribution of predicted pixels per class** matches the
distribution in the ground truth mask, regardless of their spatial location.

In simple terms, if the proportion of predicted pixels per class closely matches
the true proportions â€” even if spatially misplaced â€” the model is considered to
perform well under DPA.

Pros:
------
âœ”ï¸ Robust to small spatial misalignments
âœ”ï¸ Ideal for tasks like **food waste estimation**, where predicting â€œhow muchâ€ of
   each class exists is more important than â€œwhereâ€ it exists
âœ”ï¸ Simple and interpretable (values in [0, 1])
âœ”ï¸ Fast to compute, no spatial matching needed

Cons:
------
âœ–ï¸ Ignores spatial correctness (e.g., wrongly placed predictions are not penalized)
âœ–ï¸ Cannot evaluate boundary precision or object localization
âœ–ï¸ May report high scores for completely misplaced segmentations with correct ratios

DPA Variants:
--------------

1. **DPA-Macro**
   Each classâ€™s predicted-vs-true proportion error is treated equally, regardless of
   how many pixels that class has. Suitable when **fairness across classes** is a goal.

   Formula:
   \[
   \text{DPA}_{macro} = 1 - \frac{1}{C} \sum_{i=1}^{C} \left| \text{pred}_i - \text{gt}_i \right|
   \]
   where \( \text{pred}_i \) and \( \text{gt}_i \) are the normalized proportions of class *i*
   in prediction and ground truth, and C is the number of classes.

2. **DPA-Weighted**
   The per-class errors are weighted by the number of pixels of each class in the
   ground truth. This version reflects **realistic performance** more accurately in
   imbalanced datasets.

   Formula:
   \[
   \text{DPA}_{weighted} = 1 - \sum_{i=1}^{C} w_i \cdot \left| \text{pred}_i - \text{gt}_i \right|
   \]
   where \( w_i = \frac{\text{gt\_count}_i}{\text{total\_pixels}} \)

Warning:
---------
This metric is **not suitable** for use cases where spatial alignment and pixel-wise
localization are essential, such as:
- Medical image segmentation (e.g., tumor boundaries)
- Road/lane segmentation in autonomous driving
- Satellite or aerial imagery analysis
- Object detection/segmentation with precise contours

====================================================================================
"""

import torch

def dpa_macro(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> torch.Tensor:
    """
    Compute Distributional Pixel Agreement (DPA) using macro averaging.

    Args:
        preds (Tensor): Predicted class map of shape [H, W] or [B, H, W]
        targets (Tensor): Ground truth class map of same shape
        num_classes (int): Number of semantic classes

    Returns:
        Tensor: Scalar DPA score in range [0, 1]
    """
    device = preds.device
    preds = preds.view(-1).long()
    targets = targets.view(-1).long()

    pred_hist = torch.bincount(preds, minlength=num_classes).float().to(device)
    target_hist = torch.bincount(targets, minlength=num_classes).float().to(device)

    pred_ratio = pred_hist / (pred_hist.sum() + 1e-6)
    target_ratio = target_hist / (target_hist.sum() + 1e-6)

    abs_diff = torch.abs(pred_ratio - target_ratio)
    dpa = 1.0 - abs_diff.mean()
    return dpa


def dpa_weighted(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> torch.Tensor:
    """
    Compute Distributional Pixel Agreement (DPA) using weighted averaging by class presence.

    Args:
        preds (Tensor): Predicted class map of shape [H, W] or [B, H, W]
        targets (Tensor): Ground truth class map of same shape
        num_classes (int): Number of semantic classes

    Returns:
        Tensor: Scalar DPA score in range [0, 1]
    """
    device = preds.device
    preds = preds.view(-1).long()
    targets = targets.view(-1).long()

    pred_hist = torch.bincount(preds, minlength=num_classes).float().to(device)
    target_hist = torch.bincount(targets, minlength=num_classes).float().to(device)

    pred_ratio = pred_hist / (pred_hist.sum() + 1e-6)
    target_ratio = target_hist / (target_hist.sum() + 1e-6)
    weights = target_hist / (target_hist.sum() + 1e-6)

    abs_diff = torch.abs(pred_ratio - target_ratio)
    dpa = 1.0 - (abs_diff * weights).sum()
    return dpa

"""## **ðŸ”Ž Evaluate METRICs before Training**"""

"""
====================================================================================
ðŸ“Š Evaluation Module for Semantic Segmentation with DPA Integration
====================================================================================

This module evaluates a segmentation model on a given DataLoader using multiple
metrics. It includes both standard metrics (Pixel Accuracy, IoU, Dice) and a
custom task-specific metric: **Distributional Pixel Agreement (DPA)**.

Metrics Computed:
------------------
1. Pixel Accuracy (macro & weighted)
2. Intersection over Union (IoU) (macro & weighted)
3. Dice Coefficient (macro & weighted)
4. DPA â€“ Distributional Pixel Agreement (macro & weighted)
5. Cross-Entropy Loss

The evaluation is designed to work **before training (e.g., Epoch 0)** or during/after
training for monitoring performance. It assumes a segmentation model that outputs
logits of shape [B, C, H, W].

NOTE: The DPA metric focuses on class-wise proportion estimation, not spatial alignment,
making it ideal for tasks like food waste estimation, but unsuitable for medical,
autonomous driving, or localization-critical applications.
====================================================================================
"""

import torch
from tqdm import tqdm
from torch.nn import functional as F

# === Ensure you import or define the following BEFORE using this module ===
# pixel_accuracy_macro, pixel_accuracy_weighted
# iou_macro, iou_weighted
# dice_macro(), dice_weighted()
# dpa_macro(), dpa_weighted()


def evaluate_metrics(model, data_loader, num_classes):
    """
    Evaluate segmentation model performance on a given DataLoader.

    Args:
        model (torch.nn.Module): The segmentation model
        data_loader (DataLoader): PyTorch DataLoader for a dataset (train/valid/test)
        num_classes (int): Number of output classes for segmentation (including background)

    Returns:
        Tuple containing averaged metrics:
            - pixel_accuracy_macro
            - pixel_accuracy_weighted
            - dice_macro
            - dice_weighted
            - iou_macro
            - iou_weighted
            - dpa_macro
            - dpa_weighted
            - cross_entropy_loss
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Move metrics to device if needed
    global pixel_accuracy_macro, pixel_accuracy_weighted
    global iou_macro, iou_weighted

    pixel_accuracy_macro = pixel_accuracy_macro.to(device)
    pixel_accuracy_weighted = pixel_accuracy_weighted.to(device)
    iou_macro = iou_macro.to(device)
    iou_weighted = iou_weighted.to(device)

    model.eval()

    total_pixel_acc_macro    = 0.0
    total_pixel_acc_weighted = 0.0
    total_dice_macro         = 0.0
    total_dice_weighted      = 0.0
    total_iou_macro          = 0.0
    total_iou_weighted       = 0.0
    total_dpa_macro          = 0.0
    total_dpa_weighted       = 0.0
    total_loss               = 0.0

    num_batches = len(data_loader)

    with torch.no_grad():
        for images, masks in tqdm(data_loader, desc="Evaluating", leave=False, unit="batch"):
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)

            preds = torch.softmax(outputs, dim=1)
            preds_argmax = preds.argmax(dim=1)  # [B, H, W]

            if masks.dim() == 4 and masks.size(1) == 1:
                masks = masks.squeeze(1)
            masks = masks.long()

            # Compute Cross-Entropy Loss
            loss = F.cross_entropy(outputs, masks)
            total_loss += loss.item()

            # Compute metrics
            total_pixel_acc_macro    += pixel_accuracy_macro(preds_argmax, masks)
            total_pixel_acc_weighted += pixel_accuracy_weighted(preds_argmax, masks)
            total_dice_macro         += dice_macro(preds_argmax, masks, num_classes).to(device)
            total_dice_weighted      += dice_weighted(preds_argmax, masks, num_classes).to(device)
            total_iou_macro          += iou_macro(preds_argmax, masks)
            total_iou_weighted       += iou_weighted(preds_argmax, masks)
            total_dpa_macro          += dpa_macro(preds_argmax, masks, num_classes).to(device)
            total_dpa_weighted       += dpa_weighted(preds_argmax, masks, num_classes).to(device)

    return (
        total_pixel_acc_macro / num_batches,
        total_pixel_acc_weighted / num_batches,
        total_dice_macro / num_batches,
        total_dice_weighted / num_batches,
        total_iou_macro / num_batches,
        total_iou_weighted / num_batches,
        total_dpa_macro / num_batches,
        total_dpa_weighted / num_batches,
        total_loss / num_batches,
    )


# ======================== Example Evaluation at Epoch 0 ===========================

train_results_0 = evaluate_metrics(model, train_loader, num_classes=out_channel)
valid_results_0 = evaluate_metrics(model, valid_loader, num_classes=out_channel)

initial_metrics_epoch_0 = {
    "train_pixel_accuracy_macro":    train_results_0[0],
    "train_pixel_accuracy_weighted": train_results_0[1],
    "train_dice_macro":              train_results_0[2],
    "train_dice_weighted":           train_results_0[3],
    "train_iou_macro":               train_results_0[4],
    "train_iou_weighted":            train_results_0[5],
    "train_dpa_macro":               train_results_0[6],
    "train_dpa_weighted":            train_results_0[7],
    "train_loss":                    train_results_0[8],

    "valid_pixel_accuracy_macro":    valid_results_0[0],
    "valid_pixel_accuracy_weighted": valid_results_0[1],
    "valid_dice_macro":              valid_results_0[2],
    "valid_dice_weighted":           valid_results_0[3],
    "valid_iou_macro":               valid_results_0[4],
    "valid_iou_weighted":            valid_results_0[5],
    "valid_dpa_macro":               valid_results_0[6],
    "valid_dpa_weighted":            valid_results_0[7],
    "valid_loss":                    valid_results_0[8],
}

print("\nðŸ“Š Epoch 0 Evaluation Metrics:")
for key, value in initial_metrics_epoch_0.items():
    print(f"{key:<35}: {value:.4f}")

"""## **ðŸ“ Creating History for train and valid**"""

# ==============================================================================================================================
# ðŸ“Š Metric Tracking Initialization (Train & Validation)
# This section initializes lists to track metric values across training epochs for both train and validation datasets.

# --- Train History Lists ---
# These lists will store the metric values for each epoch to monitor the training process.
train_pixel_accuracy_macro_history    = [initial_metrics_epoch_0['train_pixel_accuracy_macro']]
train_pixel_accuracy_weighted_history = [initial_metrics_epoch_0['train_pixel_accuracy_weighted']]
train_dice_macro_history              = [initial_metrics_epoch_0['train_dice_macro']]
train_dice_weighted_history           = [initial_metrics_epoch_0['train_dice_weighted']]
train_iou_macro_history               = [initial_metrics_epoch_0['train_iou_macro']]
train_iou_weighted_history            = [initial_metrics_epoch_0['train_iou_weighted']]
train_dpa_macro_history               = [initial_metrics_epoch_0['train_dpa_macro']]
train_dpa_weighted_history            = [initial_metrics_epoch_0['train_dpa_weighted']]
train_loss_history                    = [initial_metrics_epoch_0['train_loss']]

# --- Validation History Lists ---
# These lists track how the model performs on unseen data after each epoch.
valid_pixel_accuracy_macro_history    = [initial_metrics_epoch_0['valid_pixel_accuracy_macro']]
valid_pixel_accuracy_weighted_history = [initial_metrics_epoch_0['valid_pixel_accuracy_weighted']]
valid_dice_macro_history              = [initial_metrics_epoch_0['valid_dice_macro']]
valid_dice_weighted_history           = [initial_metrics_epoch_0['valid_dice_weighted']]
valid_iou_macro_history               = [initial_metrics_epoch_0['valid_iou_macro']]
valid_iou_weighted_history            = [initial_metrics_epoch_0['valid_iou_weighted']]
valid_dpa_macro_history               = [initial_metrics_epoch_0['valid_dpa_macro']]
valid_dpa_weighted_history            = [initial_metrics_epoch_0['valid_dpa_weighted']]
valid_loss_history                    = [initial_metrics_epoch_0['valid_loss']]

# ðŸ† Best Metric Tracking
# These variables hold the best values seen for each validation metric across all epochs.
best_pixel_accuracy_macro    = 0.0
best_pixel_accuracy_weighted = 0.0
best_dice_macro              = 0.0
best_dice_weighted           = 0.0
best_iou_macro               = 0.0
best_iou_weighted            = 0.0
best_dpa_macro               = 0.0
best_dpa_weighted            = 0.0
best_loss                    = float('inf')  # Lower loss is better

# ðŸ• Epoch Tracking for Best Metrics
best_epoch_pixel_accuracy_macro    = 0
best_epoch_pixel_accuracy_weighted = 0
best_epoch_dice_macro              = 0
best_epoch_dice_weighted           = 0
best_epoch_iou_macro               = 0
best_epoch_iou_weighted            = 0
best_epoch_dpa_macro               = 0
best_epoch_dpa_weighted            = 0
best_epoch_loss                    = 0

# ðŸ’¾ Best Model Checkpoint Paths
best_model_pixel_accuracy_macro    = None
best_model_pixel_accuracy_weighted = None
best_model_dice_macro              = None
best_model_dice_weighted           = None
best_model_iou_macro               = None
best_model_iou_weighted            = None
best_model_dpa_macro               = None
best_model_dpa_weighted            = None
best_model_loss                    = None
# ==============================================================================================================================

"""## **ðŸ§® AverageMeter Class**"""

class AverageMeter:
    """Computes and stores the average and current value."""

    def __init__(self):
        self.reset()

    def reset(self):
        """Reset all the values."""
        self.val = 0  # Current value
        self.avg = 0  # Average value
        self.sum = 0  # Sum of values
        self.count = 0  # Count of values

    def update(self, val, n=1):
        """Update the meter with a new value.

        Args:
            val (float): New value to add.
            n (int): Number of occurrences of the new value (default: 1).
        """
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count  # Calculate average

"""## **ðŸ› ï¸ Functios**

### train one epoch
"""

"""
====================================================================================
ðŸ§ª train_one_epoch â€“ Training Function for One Epoch with DPA Integration
====================================================================================

Trains a segmentation model for one epoch and computes a wide range of evaluation
metrics including:
- Pixel Accuracy (macro & weighted)
- Dice Coefficient (macro & weighted)
- Intersection over Union (macro & weighted)
- DPA â€“ Distributional Pixel Agreement (macro & weighted)
- Cross-Entropy Loss

This function assumes the model outputs logits of shape [B, C, H, W] and that
the number of output classes is dynamically set (e.g., 2 for AdasPolo, 3 for others).

Arguments:
-----------
- model         : torch.nn.Module     â€” segmentation model to train
- loader        : DataLoader          â€” PyTorch DataLoader for training data
- loss_function : nn.Module           â€” loss function (e.g., CrossEntropy or custom)
- optimizer     : torch.optim.Optimizer â€” optimizer for updating model parameters
- num_classes   : int                 â€” number of segmentation classes (== out_channels)

Returns:
---------
Tuple of average metrics for this epoch
====================================================================================
"""

def train_one_epoch(model, loader, loss_function, optimizer, num_classes):
    model.train()

    # Metric trackers
    loss_meter = AverageMeter()
    pixel_acc_macro_meter = AverageMeter()
    pixel_acc_weighted_meter = AverageMeter()
    dice_macro_meter = AverageMeter()
    dice_weighted_meter = AverageMeter()
    iou_macro_meter = AverageMeter()
    iou_weighted_meter = AverageMeter()
    dpa_macro_meter = AverageMeter()
    dpa_weighted_meter = AverageMeter()

    for images, masks in tqdm(loader, desc="Training", unit="batches"):
        images, masks = images.to(device), masks.to(device)
        if masks.dim() == 4 and masks.size(1) == 1:
            masks = masks.squeeze(1)
        masks = masks.long()

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_function(outputs, masks)
        loss.backward()
        optimizer.step()

        preds = torch.softmax(outputs, dim=1).argmax(dim=1)

        # Update metrics
        loss_meter.update(loss.item(), images.size(0))
        pixel_acc_macro_meter.update(pixel_accuracy_macro(preds, masks), images.size(0))
        pixel_acc_weighted_meter.update(pixel_accuracy_weighted(preds, masks), images.size(0))
        dice_macro_meter.update(dice_macro(preds, masks, num_classes), images.size(0))
        dice_weighted_meter.update(dice_weighted(preds, masks, num_classes), images.size(0))
        iou_macro_meter.update(iou_macro(preds, masks), images.size(0))
        iou_weighted_meter.update(iou_weighted(preds, masks), images.size(0))
        dpa_macro_meter.update(dpa_macro(preds, masks, num_classes), images.size(0))
        dpa_weighted_meter.update(dpa_weighted(preds, masks, num_classes), images.size(0))

    return (
        loss_meter.avg,
        pixel_acc_macro_meter.avg,
        pixel_acc_weighted_meter.avg,
        dice_macro_meter.avg,
        dice_weighted_meter.avg,
        iou_macro_meter.avg,
        iou_weighted_meter.avg,
        dpa_macro_meter.avg,
        dpa_weighted_meter.avg
    )

"""### valid one epoch"""

"""
====================================================================================
ðŸ“Š valid_one_epoch â€“ Validation Function for One Epoch with DPA Integration
====================================================================================

Validates a segmentation model for one epoch and evaluates the following metrics:
- Pixel Accuracy (macro & weighted)
- Dice Coefficient (macro & weighted)
- Intersection over Union (macro & weighted)
- DPA â€“ Distributional Pixel Agreement (macro & weighted)
- Cross-Entropy Loss

Arguments:
-----------
- model         : torch.nn.Module     â€” segmentation model to evaluate
- loader        : DataLoader          â€” PyTorch DataLoader for validation data
- loss_function : nn.Module           â€” loss function (e.g., CrossEntropy or custom)
- num_classes   : int                 â€” number of segmentation classes (== out_channels)

Returns:
---------
Tuple of average metrics for this epoch
====================================================================================
"""

def valid_one_epoch(model, loader, loss_function, num_classes):
    model.eval()

    # Metric trackers
    loss_meter = AverageMeter()
    pixel_acc_macro_meter = AverageMeter()
    pixel_acc_weighted_meter = AverageMeter()
    dice_macro_meter = AverageMeter()
    dice_weighted_meter = AverageMeter()
    iou_macro_meter = AverageMeter()
    iou_weighted_meter = AverageMeter()
    dpa_macro_meter = AverageMeter()
    dpa_weighted_meter = AverageMeter()

    with torch.no_grad():
        for images, masks in tqdm(loader, desc="Validation", unit="batches"):
            images, masks = images.to(device), masks.to(device)
            if masks.dim() == 4 and masks.size(1) == 1:
                masks = masks.squeeze(1)
            masks = masks.long()

            outputs = model(images)
            loss = loss_function(outputs, masks)

            preds = torch.softmax(outputs, dim=1).argmax(dim=1)

            # Update metrics
            loss_meter.update(loss.item(), images.size(0))
            pixel_acc_macro_meter.update(pixel_accuracy_macro(preds, masks), images.size(0))
            pixel_acc_weighted_meter.update(pixel_accuracy_weighted(preds, masks), images.size(0))
            dice_macro_meter.update(dice_macro(preds, masks, num_classes), images.size(0))
            dice_weighted_meter.update(dice_weighted(preds, masks, num_classes), images.size(0))
            iou_macro_meter.update(iou_macro(preds, masks), images.size(0))
            iou_weighted_meter.update(iou_weighted(preds, masks), images.size(0))
            dpa_macro_meter.update(dpa_macro(preds, masks, num_classes), images.size(0))
            dpa_weighted_meter.update(dpa_weighted(preds, masks, num_classes), images.size(0))

    return (
        loss_meter.avg,
        pixel_acc_macro_meter.avg,
        pixel_acc_weighted_meter.avg,
        dice_macro_meter.avg,
        dice_weighted_meter.avg,
        iou_macro_meter.avg,
        iou_weighted_meter.avg,
        dpa_macro_meter.avg,
        dpa_weighted_meter.avg
    )

"""## **âœ…One more model CHECK**

![model summary.png](data:image/png;base64,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)
"""

import torch

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters()) / 1e6
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6

# Display counts in millions
print(f"ðŸ§  Total Parameters:     {total_params:.1f}M")
print(f"ðŸ› ï¸ Trainable Parameters: {trainable_params:.1f}M")
print()

"""## **ðŸ”„Training Loop + Num epochs**"""

"""
Cell 15: âœ… Set input/output channels for segmentation based on food category

This cell ensures consistent variable naming across the entire project:
- Input channels are fixed to 3 (RGB), assigned to `in_channels` and `in_channel`
- Output channels depend on the selected food category, and are assigned consistently to:
  `out_channels`, `out_channel`, `num_classes`, and `num_class`
"""

# === Set input channels ===
in_channels = in_channel = 3  # RGB images

# === Set output channels based on food class ===
if selected_category == "AdasPolo":
    out_channels = out_channel = num_classes = num_class = 2  # Background + AdasPolo
elif selected_category in ["CheloGoosht", "Fesenjan", "GheymeBademjan", "Protein_and_Fries"]:
    out_channels = out_channel = num_classes = num_class = 3  # Background + 2 foregrounds
else:
    raise ValueError(f"âŒ Unknown food category: {selected_category}")

# === Confirm final config ===
print("âœ… Configuration Set")
print(f"in_channels   = {in_channels}")
print(f"in_channel    = {in_channel} ")
print(f"out_channels  = {out_channels}")
print(f"out_channel   = {out_channel}")
print(f"num_classes   = {num_classes}")
print(f"num_class     = {num_class}")
print(f"ðŸ”Ž Category    = {selected_category}")

print("")
print(f"Initial LR is {lr}")

# macro    => unweighted
# weighted => weighted

num_epochs = 25

"""
====================================================================================
ðŸ” Epoch-wise Training Loop with DPA Integration
====================================================================================

This loop performs epoch-wise training and validation for a segmentation model.
It tracks and prints key metrics, emphasizing **weighted metrics**, as they better
reflect real-world proportional relevance in tasks like food waste estimation.

Metrics included:
- Pixel Accuracy (macro & weighted)
- IoU (macro & weighted)
- Dice (macro & weighted)
- DPA (macro & weighted)
- Cross-entropy loss
====================================================================================
"""

print("ðŸ“Œ Note: In this task, **weighted metrics** are prioritized due to their relevance to class distribution.")
print("Macro metrics are reported for completeness, but are not significant to the evaluation logic.\n")

for epoch in range(num_epochs):
    print(f"ðŸ§ª Epoch {epoch + 1}/{num_epochs}")

    # Train and validate (now with num_classes)
    train_loss, train_pixel_acc_macro, train_pixel_acc_weighted, train_dice_macro, train_dice_weighted, train_iou_macro, train_iou_weighted, train_dpa_macro, train_dpa_weighted = train_one_epoch(
        model, train_loader, loss_function, optimizer, num_classes)

    valid_loss, valid_pixel_acc_macro, valid_pixel_acc_weighted, valid_dice_macro, valid_dice_weighted, valid_iou_macro, valid_iou_weighted, valid_dpa_macro, valid_dpa_weighted = valid_one_epoch(
        model, valid_loader, loss_function, num_classes)

    # Append training history
    train_pixel_accuracy_macro_history.append(train_pixel_acc_macro)
    train_pixel_accuracy_weighted_history.append(train_pixel_acc_weighted)
    train_dice_macro_history.append(train_dice_macro)
    train_dice_weighted_history.append(train_dice_weighted)
    train_iou_macro_history.append(train_iou_macro)
    train_iou_weighted_history.append(train_iou_weighted)
    train_dpa_macro_history.append(train_dpa_macro)
    train_dpa_weighted_history.append(train_dpa_weighted)
    train_loss_history.append(train_loss)

    # Append validation history
    valid_pixel_accuracy_macro_history.append(valid_pixel_acc_macro)
    valid_pixel_accuracy_weighted_history.append(valid_pixel_acc_weighted)
    valid_dice_macro_history.append(valid_dice_macro)
    valid_dice_weighted_history.append(valid_dice_weighted)
    valid_iou_macro_history.append(valid_iou_macro)
    valid_iou_weighted_history.append(valid_iou_weighted)
    valid_dpa_macro_history.append(valid_dpa_macro)
    valid_dpa_weighted_history.append(valid_dpa_weighted)
    valid_loss_history.append(valid_loss)

    # Print metrics for this epoch
    print("âœ… Weighted Metrics (Key focus in this study):")
    print(f"Train - LOSS: {train_loss:.4f} , PixelAcc: {train_pixel_acc_weighted:.4f}, IoU: {train_iou_weighted:.4f}, Dice: {train_dice_weighted:.4f}, DPA: {train_dpa_weighted:.4f}")
    print(f"Valid - LOSS: {valid_loss:.4f} , PixelAcc: {valid_pixel_acc_weighted:.4f}, IoU: {valid_iou_weighted:.4f}, Dice: {valid_dice_weighted:.4f}, DPA: {valid_dpa_weighted:.4f}\n")

    print("ðŸ“Š Macro Metrics (Reported, but not prioritized):")
    print(f"Train - PixelAcc: {train_pixel_acc_macro:.4f}, IoU: {train_iou_macro:.4f}, Dice: {train_dice_macro:.4f}, DPA: {train_dpa_macro:.4f}")
    print(f"Valid - PixelAcc: {valid_pixel_acc_macro:.4f}, IoU: {valid_iou_macro:.4f}, Dice: {valid_dice_macro:.4f}, DPA: {valid_dpa_macro:.4f}\n")

    # Print learning rate
    current_lr = optimizer.param_groups[0]['lr']
    print(f"ðŸ“‰ Learning Rate: {current_lr:.6f}\n")

    # Best-model tracking
    if valid_pixel_acc_macro > best_pixel_accuracy_macro:
        best_pixel_accuracy_macro = valid_pixel_acc_macro
        best_epoch_pixel_accuracy_macro = epoch + 1
        best_model_pixel_accuracy_macro = model.state_dict()

    if valid_pixel_acc_weighted > best_pixel_accuracy_weighted:
        best_pixel_accuracy_weighted = valid_pixel_acc_weighted
        best_epoch_pixel_accuracy_weighted = epoch + 1
        best_model_pixel_accuracy_weighted = model.state_dict()

    if valid_dice_macro > best_dice_macro:
        best_dice_macro = valid_dice_macro
        best_epoch_dice_macro = epoch + 1
        best_model_dice_macro = model.state_dict()

    if valid_dice_weighted > best_dice_weighted:
        best_dice_weighted = valid_dice_weighted
        best_epoch_dice_weighted = epoch + 1
        best_model_dice_weighted = model.state_dict()

    if valid_iou_macro > best_iou_macro:
        best_iou_macro = valid_iou_macro
        best_epoch_iou_macro = epoch + 1
        best_model_iou_macro = model.state_dict()

    if valid_iou_weighted > best_iou_weighted:
        best_iou_weighted = valid_iou_weighted
        best_epoch_iou_weighted = epoch + 1
        best_model_iou_weighted = model.state_dict()

    if valid_dpa_macro > best_dpa_macro:
        best_dpa_macro = valid_dpa_macro
        best_epoch_dpa_macro = epoch + 1
        best_model_dpa_macro = model.state_dict()

    if valid_dpa_weighted > best_dpa_weighted:
        best_dpa_weighted = valid_dpa_weighted
        best_epoch_dpa_weighted = epoch + 1
        best_model_dpa_weighted = model.state_dict()

    if valid_loss < best_loss:
        best_loss = valid_loss
        best_epoch_loss = epoch + 1
        best_model_loss = model.state_dict()

    # ðŸ” Step the learning rate scheduler
    scheduler.step()  # âœ… Correct!

"""## **ðŸ”Ž Performance Evaluation (based on Validation!)**

### Epoch based
"""

# ======================================================================================
# ðŸ“Š Summary of Best Validation Metrics (After Training)
# Note: These are based on validation performance, not training
# ======================================================================================

# Pixel Accuracy
print(f"Best Pixel Accuracy (Macro):      {best_pixel_accuracy_macro:.4f}       at Epoch    {best_epoch_pixel_accuracy_macro}")
print(f"Best Pixel Accuracy (Weighted):   {best_pixel_accuracy_weighted:.4f}    at Epoch    {best_epoch_pixel_accuracy_weighted}")
print("")

# IoU
print(f"Best IoU (Macro):                 {best_iou_macro:.4f}                  at Epoch    {best_epoch_iou_macro}")
print(f"Best IoU (Weighted):              {best_iou_weighted:.4f}               at Epoch    {best_epoch_iou_weighted}")
print("")

# Dice Coefficient
print(f"Best Dice Coefficient (Macro):    {best_dice_macro:.4f}                at Epoch    {best_epoch_dice_macro}")
print(f"Best Dice Coefficient (Weighted): {best_dice_weighted:.4f}             at Epoch    {best_epoch_dice_weighted}")
print("")

# DPA â€“ Distributional Pixel Agreement
print(f"Best DPA (Macro):                 {best_dpa_macro:.4f}                  at Epoch    {best_epoch_dpa_macro}")
print(f"Best DPA (Weighted):              {best_dpa_weighted:.4f}               at Epoch    {best_epoch_dpa_weighted}")
print("")

# Loss
print(f"Best (Least) Cross-Entropy Loss:  {best_loss:.4f}                      at Epoch    {best_epoch_loss}")
print("=====================================================================================\n")

"""### Graph based

#### Pixel ACC
"""

# ======================================
# Pixel Accuracy Plot (Macro & Weighted)
# ======================================
min_len_pa = min(len(train_pixel_accuracy_weighted_history), len(valid_pixel_accuracy_weighted_history),
                 len(train_pixel_accuracy_macro_history), len(valid_pixel_accuracy_macro_history))
epochs_range_pa = range(min_len_pa)

fig, axs = plt.subplots(1, 2, figsize=(18, 6))

axs[0].plot(epochs_range_pa, [x.cpu().numpy() for x in train_pixel_accuracy_weighted_history[:min_len_pa]], 'o-', color='green', label='Train Pixel Accuracy (Weighted)')
axs[0].plot(epochs_range_pa, [x.cpu().numpy() for x in valid_pixel_accuracy_weighted_history[:min_len_pa]], 's-', color='red', label='Valid Pixel Accuracy (Weighted)')
axs[0].set_title('Weighted Pixel Accuracy Over Epochs')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Weighted Pixel Accuracy')
axs[0].set_yticks([i * 0.1 for i in range(11)])
axs[0].set_xticks(epochs_range_pa)
axs[0].legend()
axs[0].grid(True)

axs[1].plot(epochs_range_pa, [x.cpu().numpy() for x in train_pixel_accuracy_macro_history[:min_len_pa]], 'o-', color='blue', label='Train Pixel Accuracy (Macro)')
axs[1].plot(epochs_range_pa, [x.cpu().numpy() for x in valid_pixel_accuracy_macro_history[:min_len_pa]], 's-', color='orange', label='Valid Pixel Accuracy (Macro)')
axs[1].set_title('Macro Pixel Accuracy Over Epochs')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Macro Pixel Accuracy')
axs[1].set_yticks([i * 0.1 for i in range(11)])
axs[1].set_xticks(epochs_range_pa)
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()

"""#### IoU"""

# ==========================
# IoU Plot (Macro & Weighted)
# ==========================
min_len_iou = min(len(train_iou_weighted_history), len(valid_iou_weighted_history),
                  len(train_iou_macro_history), len(valid_iou_macro_history))
epochs_range_iou = range(min_len_iou)

fig, axs = plt.subplots(1, 2, figsize=(18, 6))

axs[0].plot(epochs_range_iou, [x.cpu().numpy() for x in train_iou_weighted_history[:min_len_iou]], 'o-', color='green', label='Train IoU (Weighted)')
axs[0].plot(epochs_range_iou, [x.cpu().numpy() for x in valid_iou_weighted_history[:min_len_iou]], 's-', color='red', label='Valid IoU (Weighted)')
axs[0].set_title('Weighted IoU Over Epochs')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Weighted IoU')
axs[0].set_yticks([i * 0.1 for i in range(11)])
axs[0].set_xticks(epochs_range_iou)
axs[0].legend()
axs[0].grid(True)

axs[1].plot(epochs_range_iou, [x.cpu().numpy() for x in train_iou_macro_history[:min_len_iou]], 'o-', color='blue', label='Train IoU (Macro)')
axs[1].plot(epochs_range_iou, [x.cpu().numpy() for x in valid_iou_macro_history[:min_len_iou]], 's-', color='orange', label='Valid IoU (Macro)')
axs[1].set_title('Macro IoU Over Epochs')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Macro IoU')
axs[1].set_yticks([i * 0.1 for i in range(11)])
axs[1].set_xticks(epochs_range_iou)
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()

"""#### Dice"""

# =============================
# Dice Plot (Macro & Weighted)
# =============================
min_len_dice = min(len(train_dice_weighted_history), len(valid_dice_weighted_history),
                   len(train_dice_macro_history), len(valid_dice_macro_history))
epochs_range_dice = range(min_len_dice)

fig, axs = plt.subplots(1, 2, figsize=(18, 6))

axs[0].plot(epochs_range_dice, [x.cpu().numpy() for x in train_dice_weighted_history[:min_len_dice]], 'o-', color='green', label='Train Dice (Weighted)')
axs[0].plot(epochs_range_dice, [x.cpu().numpy() for x in valid_dice_weighted_history[:min_len_dice]], 's-', color='red', label='Valid Dice (Weighted)')
axs[0].set_title('Weighted Dice Coefficient Over Epochs')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Weighted Dice')
axs[0].set_yticks([i * 0.1 for i in range(11)])
axs[0].set_xticks(epochs_range_dice)
axs[0].legend()
axs[0].grid(True)

axs[1].plot(epochs_range_dice, [x.cpu().numpy() for x in train_dice_macro_history[:min_len_dice]], 'o-', color='blue', label='Train Dice (Macro)')
axs[1].plot(epochs_range_dice, [x.cpu().numpy() for x in valid_dice_macro_history[:min_len_dice]], 's-', color='orange', label='Valid Dice (Macro)')
axs[1].set_title('Macro Dice Coefficient Over Epochs')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Macro Dice')
axs[1].set_yticks([i * 0.1 for i in range(11)])
axs[1].set_xticks(epochs_range_dice)
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()

"""#### LOSS"""

# =================
# Loss Plot (Train & Valid)
# =================
min_len_loss = min(len(train_loss_history), len(valid_loss_history))
epochs_range_loss = range(min_len_loss)

plt.figure(figsize=(10, 6))
plt.plot(epochs_range_loss, train_loss_history[:min_len_loss], 'o-', color='blue', label='Train Loss')
plt.plot(epochs_range_loss, valid_loss_history[:min_len_loss], 's-', color='orange', label='Valid Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.xticks(epochs_range_loss)
plt.legend()
plt.grid()
plt.show()

"""#### DPA"""

# ========================
# New Metric: DPA Plot
# ========================
min_len_dpa = min(len(train_dpa_weighted_history), len(valid_dpa_weighted_history),
                  len(train_dpa_macro_history), len(valid_dpa_macro_history))
epochs_range_dpa = range(min_len_dpa)

fig, axs = plt.subplots(1, 2, figsize=(18, 6))

axs[0].plot(epochs_range_dpa, [x.cpu().numpy() for x in train_dpa_weighted_history[:min_len_dpa]], 'o-', color='green', label='Train DPA (Weighted)')
axs[0].plot(epochs_range_dpa, [x.cpu().numpy() for x in valid_dpa_weighted_history[:min_len_dpa]], 's-', color='red', label='Valid DPA (Weighted)')
axs[0].set_title('Weighted DPA Over Epochs')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Weighted DPA')
axs[0].set_yticks([i * 0.1 for i in range(11)])
axs[0].set_xticks(epochs_range_dpa)
axs[0].legend()
axs[0].grid(True)

axs[1].plot(epochs_range_dpa, [x.cpu().numpy() for x in train_dpa_macro_history[:min_len_dpa]], 'o-', color='blue', label='Train DPA (Macro)')
axs[1].plot(epochs_range_dpa, [x.cpu().numpy() for x in valid_dpa_macro_history[:min_len_dpa]], 's-', color='orange', label='Valid DPA (Macro)')
axs[1].set_title('Macro DPA Over Epochs')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Macro DPA')
axs[1].set_yticks([i * 0.1 for i in range(11)])
axs[1].set_xticks(epochs_range_dpa)
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()

"""### Visualization based"""

import torch
import matplotlib.pyplot as plt
import numpy as np

# Load the best model (based on Weighted IoU)
model.load_state_dict(best_model_iou_weighted)
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Get two batches from validation loader
dataiter = iter(valid_loader)
images1, masks1 = next(dataiter)
images2, masks2 = next(dataiter)

# Move to device
images1, masks1 = images1.to(device), masks1.to(device)
images2, masks2 = images2.to(device), masks2.to(device)

# Predict
with torch.no_grad():
    outputs1 = model(images1)
    outputs2 = model(images2)

predictions1 = torch.softmax(outputs1, dim=1).argmax(dim=1)
predictions2 = torch.softmax(outputs2, dim=1).argmax(dim=1)

# Convert to numpy
images_np1 = images1.cpu().numpy()
masks_np1 = masks1.cpu().numpy()
predictions_np1 = predictions1.cpu().numpy()

images_np2 = images2.cpu().numpy()
masks_np2 = masks2.cpu().numpy()
predictions_np2 = predictions2.cpu().numpy()

# Plotting
fig, axes = plt.subplots(nrows=4, ncols=10, figsize=(20, 9))

def plot_row(axes, row_idx, image, mask, prediction, col_offset=0):
    img_disp = image.transpose(1, 2, 0)
    img_disp = np.clip(img_disp, 0, 1)

    # Original image
    axes[row_idx, col_offset].imshow(img_disp)
    axes[row_idx, col_offset].axis('off')
    if row_idx == 0:
        axes[row_idx, col_offset].set_title('Original')

    # Ground truth
    axes[row_idx, col_offset + 1].imshow(mask.squeeze(), cmap='tab20', vmin=0, vmax=10)
    axes[row_idx, col_offset + 1].axis('off')
    if row_idx == 0:
        axes[row_idx, col_offset + 1].set_title('Ground Truth')

    # Prediction
    axes[row_idx, col_offset + 2].imshow(prediction.squeeze(), cmap='tab20', vmin=0, vmax=10)
    axes[row_idx, col_offset + 2].axis('off')
    if row_idx == 0:
        axes[row_idx, col_offset + 2].set_title('Prediction')

    # Overlay with Ground Truth
    mask_rgb = plt.cm.tab20(mask.squeeze() / 10)[:, :, :3]
    overlay_gt = 0.3 * img_disp + 0.7 * mask_rgb
    axes[row_idx, col_offset + 3].imshow(overlay_gt)
    axes[row_idx, col_offset + 3].axis('off')
    if row_idx == 0:
        axes[row_idx, col_offset + 3].set_title('Overlay (Ground Truth)')

    # Overlay with Prediction
    pred_rgb = plt.cm.tab20(prediction / 10)[:, :, :3]
    overlay_pred = 0.3 * img_disp + 0.7 * pred_rgb
    axes[row_idx, col_offset + 4].imshow(overlay_pred)
    axes[row_idx, col_offset + 4].axis('off')
    if row_idx == 0:
        axes[row_idx, col_offset + 4].set_title('Overlay (Prediction)')

# Plot first batch (left half)
for i in range(4):
    plot_row(axes, i, images_np1[i], masks_np1[i], predictions_np1[i], col_offset=0)

# Plot second batch (right half)
for i in range(4):
    plot_row(axes, i, images_np2[i], masks_np2[i], predictions_np2[i], col_offset=5)

plt.tight_layout()
plt.savefig("Validation_DATA_Visualization_Overlay_GroundTruth.png", dpi=800, bbox_inches='tight')
plt.show()

"""## **ðŸ”Žâœ…Performance based on Test Data**

### Based on numbers
"""

# =========================== Cell: Evaluate Best Model on Test Data ===========================
"""
Evaluates the best saved model (based on weighted IoU) on the held-out test set,
reporting both weighted and macro metrics including Pixel Accuracy, IoU, Dice, and DPA.
"""

# Load the best model based on weighted IoU
model.load_state_dict(best_model_iou_weighted)
model.to(device)
model.eval()

# Initialize accumulators
pixel_acc_weighted_total = 0.0
iou_weighted_total = 0.0
dice_weighted_total = 0.0
dpa_weighted_total = 0.0

pixel_acc_macro_total = 0.0
iou_macro_total = 0.0
dice_macro_total = 0.0
dpa_macro_total = 0.0

total_samples = 0

# Evaluate on test set
with torch.no_grad():
    for images, masks in test_loader:
        images, masks = images.to(device), masks.to(device)

        if masks.dim() == 4 and masks.size(1) == 1:
            masks = masks.squeeze(1)
        masks = masks.long()

        outputs = model(images)
        preds = torch.softmax(outputs, dim=1).argmax(dim=1)

        batch_size = images.size(0)
        total_samples += batch_size

        # Weighted metrics
        pixel_acc_weighted_total += pixel_accuracy_weighted(preds, masks) * batch_size
        iou_weighted_total += iou_weighted(preds, masks) * batch_size
        dice_weighted_total += dice_weighted(preds, masks, num_classes) * batch_size
        dpa_weighted_total += dpa_weighted(preds, masks, num_classes) * batch_size

        # Macro metrics
        pixel_acc_macro_total += pixel_accuracy_macro(preds, masks) * batch_size
        iou_macro_total += iou_macro(preds, masks) * batch_size
        dice_macro_total += dice_macro(preds, masks, num_classes) * batch_size
        dpa_macro_total += dpa_macro(preds, masks, num_classes) * batch_size

# Compute averages
pixel_acc_weighted_test = pixel_acc_weighted_total / total_samples
iou_weighted_test = iou_weighted_total / total_samples
dice_weighted_test = dice_weighted_total / total_samples
dpa_weighted_test = dpa_weighted_total / total_samples

pixel_acc_macro_test = pixel_acc_macro_total / total_samples
iou_macro_test = iou_macro_total / total_samples
dice_macro_test = dice_macro_total / total_samples
dpa_macro_test = dpa_macro_total / total_samples

# Print final test metrics
print("ðŸ“Š Final Test Evaluation with Best IoU-Weighted Model")
print("ðŸ“Œ Note: In this study, **weighted metrics** are prioritized due to class imbalance relevance.\n")

print(f"âœ… Weighted  - Pixel Acc: {pixel_acc_weighted_test:.6f}, IoU: {iou_weighted_test:.6f}, Dice: {dice_weighted_test:.6f}, DPA: {dpa_weighted_test:.6f}")
print(f"ðŸ“Š Macro     - Pixel Acc: {pixel_acc_macro_test:.6f}, IoU: {iou_macro_test:.6f}, Dice: {dice_macro_test:.6f}, DPA: {dpa_macro_test:.6f}")

"""### Based on Visualization"""

# =========================== Cell 2: Visualize Segmentation on Test Data (with Dual Overlays) ===========================

import matplotlib.pyplot as plt
import numpy as np
import torch

# Load best model based on weighted IoU
model.load_state_dict(best_model_iou_weighted)
model.to(device)
model.eval()

# Get two batches from test_loader
dataiter = iter(test_loader)
images1, masks1 = next(dataiter)
images2, masks2 = next(dataiter)

# Move data to device
images1, masks1 = images1.to(device), masks1.to(device)
images2, masks2 = images2.to(device), masks2.to(device)

# Run inference
with torch.no_grad():
    outputs1 = model(images1)
    outputs2 = model(images2)

# Get predicted masks via argmax of softmax
preds1 = torch.softmax(outputs1, dim=1).argmax(dim=1)
preds2 = torch.softmax(outputs2, dim=1).argmax(dim=1)

# Convert to NumPy arrays
images_np1 = images1.cpu().numpy()
masks_np1 = masks1.cpu().numpy()
preds_np1 = preds1.cpu().numpy()

images_np2 = images2.cpu().numpy()
masks_np2 = masks2.cpu().numpy()
preds_np2 = preds2.cpu().numpy()

# Plotting: 4 rows Ã— 10 columns (5 plots per sample Ã— 2 batches)
fig, axes = plt.subplots(4, 10, figsize=(22, 9))

# Plot function with dual overlays
def plot_row(axs, row_idx, image, mask, pred, col_offset=0):
    img_disp = image.transpose(1, 2, 0)
    img_disp = np.clip(img_disp, 0, 1)

    # Original
    axs[row_idx, col_offset].imshow(img_disp)
    axs[row_idx, col_offset].axis('off')
    if row_idx == 0:
        axs[row_idx, col_offset].set_title('Original')

    # Ground Truth Mask
    axs[row_idx, col_offset + 1].imshow(mask.squeeze(), cmap='tab20', vmin=0, vmax=num_classes - 1)
    axs[row_idx, col_offset + 1].axis('off')
    if row_idx == 0:
        axs[row_idx, col_offset + 1].set_title('GT Mask')

    # Prediction Mask
    axs[row_idx, col_offset + 2].imshow(pred.squeeze(), cmap='tab20', vmin=0, vmax=num_classes - 1)
    axs[row_idx, col_offset + 2].axis('off')
    if row_idx == 0:
        axs[row_idx, col_offset + 2].set_title('Prediction')

    # Overlay with Ground Truth
    gt_rgb = plt.cm.tab20(mask.squeeze() / max(1, num_classes - 1))[:, :, :3]
    overlay_gt = 0.3 * img_disp + 0.7 * gt_rgb
    axs[row_idx, col_offset + 3].imshow(overlay_gt)
    axs[row_idx, col_offset + 3].axis('off')
    if row_idx == 0:
        axs[row_idx, col_offset + 3].set_title('Overlay (GT)')

    # Overlay with Prediction
    pred_rgb = plt.cm.tab20(pred.squeeze() / max(1, num_classes - 1))[:, :, :3]
    overlay_pred = 0.3 * img_disp + 0.7 * pred_rgb
    axs[row_idx, col_offset + 4].imshow(overlay_pred)
    axs[row_idx, col_offset + 4].axis('off')
    if row_idx == 0:
        axs[row_idx, col_offset + 4].set_title('Overlay (Pred)')

# Plot batch 1 (columns 0â€“4)
for i in range(4):
    plot_row(axes, i, images_np1[i], masks_np1[i], preds_np1[i], col_offset=0)

# Plot batch 2 (columns 5â€“9)
for i in range(4):
    plot_row(axes, i, images_np2[i], masks_np2[i], preds_np2[i], col_offset=5)

plt.tight_layout()
plt.savefig("TEST_DATA_Visualization_Overlay_GroundTruth.png", dpi=800, bbox_inches='tight')
plt.show()